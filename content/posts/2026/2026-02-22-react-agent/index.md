---
title: "ğŸ”„ Apprendre Ã  son agent Ã  rÃ©flÃ©chir avec le pattern ReAct et LangChain4j ğŸ¤–"
description: "Iâ€™m not a man. Iâ€™m something that evolved. @Skynet"
link: /2026-02-22-react-agent
image: cover.jpg
figCaption: Â©Hemdale
tags: 
  - Code
  - IA
author: wildagsx
---

## TL;DR
> ğŸ”„ ImplÃ©mentation du pattern ReAct (Reasoning and Acting) avec LangChain4j  
> ğŸï¸ Cas d'usage : gÃ©nÃ©ration d'images avec Stable Diffusion XL  
> ğŸ” Utilisation du loop builder de [LangChain4j](https://docs.langchain4j.dev/) pour boucler entre agents  
> ğŸ™ Le [code source](https://gist.github.com/philippart-s/fdfbddbfb20bd795563dadad66315f05) du gist  

<br/>

# ğŸ“œ PrÃ©cÃ©demment dans le monde des agents...

Dans mon [prÃ©cÃ©dent article]({site.url}/2026-01-22-ai-agents) sur les agents, je mentionnais le pattern ReAct sans pour autant entrer dans les dÃ©tails de son implÃ©mentation.
Je vous propose, dans cet article, de dÃ©couvrir comment implÃ©menter ce pattern avec [LangChain4j](https://docs.langchain4j.dev/).

Je ne vais donc pas revenir sur les bases de ce qu'est un agent (pour Ã§a, je vous invite Ã  lire le prÃ©cÃ©dent article).

## ğŸ”„ Le pattern ReAct

ReAct, pour Reasoning and Acting, c'est l'idÃ©e d'introduire une **boucle de feedback** entre votre agent et son LLM.
L'objectif : maximiser la qualitÃ© de la rÃ©ponse en permettant au LLM d'itÃ©rer sur son propre travail.

En rÃ©sumÃ©, le LLM ne se contente pas de rÃ©pondre en une fois, il **raisonne**, **agit**, **observe** le rÃ©sultat, et **recommence** si nÃ©cessaire.

![](./react-agents.png)

Pour rappel :
- L'agent (ğŸ¤–) envoie la liste des outils (ğŸ› ï¸) et documents potentiellement utilisables par le LLM (ğŸ“œ), en plus de la demande (prompt)  
- Le LLM commence Ã  Ã©laborer son analyse (ğŸ’­) pour rÃ©pondre au mieux au prompt
- Si besoin, le LLM dÃ©clenche une nouvelle boucle (ğŸ”) d'Ã©change avec l'agent pour affiner son analyse
- Le LLM _estime_ avoir trouvÃ© la rÃ©ponse (âœ…), elle est renvoyÃ©e Ã  l'utilisatrice / utilisateur (ğŸ¥³)  
- Alternative : le LLM ne parvient pas Ã  aller au bout de son raisonnement (âŒ)  

# ğŸï¸ Mon use case : la gÃ©nÃ©ration d'images avec Stable Diffusion XL

Pour illustrer le pattern ReAct, j'ai choisi un cas d'usage plutÃ´t fun : la **gÃ©nÃ©ration d'images** avec Stable Diffusion XL.

> J'utilise le SDXL mis Ã  disposition par OVHcloud via [AI Endpoints](https://www.ovhcloud.com/en/public-cloud/ai-endpoints/catalog/stable-diffusion-xl/).

L'idÃ©e est simple : vous dÃ©crivez une image en langage naturel, et une boucle d'agents va :
1. **ğŸ“ Raffiner** votre description en un prompt optimisÃ© pour Stable Diffusion XL
2. **ğŸï¸ GÃ©nÃ©rer** l'image avec ce prompt
3. **ğŸ” Critiquer** l'image gÃ©nÃ©rÃ©e (via un modÃ¨le de vision)
4. **ğŸ”„ Recommencer** si la critique n'est pas satisfaisante (score < 0.8)

# ğŸ¤– Les agents

## ğŸ“ PromptRefiner : l'expert en prompt SDXL

Le premier agent est un spÃ©cialiste du prompt engineering pour Stable Diffusion XL.
Son rÃ´le : prendre la description de l'utilisateur (et Ã©ventuellement le feedback du critique) et produire un prompt optimisÃ©.

CommenÃ§ons par le record qui va porter le rÃ©sultat de cet agent :

{|
```java
public record SdxlPrompts(String prompt, String negativePrompt) {
}
```
|}

Rien de fou, un prompt positif (ce qu'on veut) et un prompt nÃ©gatif (ce qu'on ne veut pas).
Si vous n'Ãªtes pas familier avec Stable Diffusion, le negative prompt permet d'exclure des artefacts indÃ©sirables (flou, mauvaise anatomie, etc.).

Et voici l'agent lui-mÃªme :

{|
```java
public interface PromptRefiner {
  @SystemMessage("""
      You are an expert prompt engineer for Stable Diffusion XL.
      Your job is to create or refine a detailed prompt and negative prompt for image generation.
      When given feedback from a critic, incorporate that feedback to improve the prompts.
      Respond with ONLY a JSON object (no markdown, no code fences) in this exact format:
      {"prompt": "detailed SDXL prompt here", "negativePrompt": "negative prompt here"}
      The prompt should be highly detailed with style, lighting, quality keywords.
      The negative prompt should exclude common artifacts and unwanted elements.
      """)
  @Agent(description = "Creates or refines Stable Diffusion XL prompts from a user request and optional critic feedback", outputKey = "sdxlPrompts")
  @UserMessage("""
      User request: "{{userRequest}}"
      Previous critic feedback: "{{feedback}}"
      Create optimized Stable Diffusion XL prompts for this request.
      """)
  SdxlPrompts refinePrompt(@V("userRequest") String userRequest, @V("feedback") String feedback);
}
```
|}

On retrouve l'annotation `@Agent` avec son `outputKey` qui permet de stocker le rÃ©sultat dans le [contexte agentique](https://docs.langchain4j.dev/tutorials/agents#introducing-the-agenticscope).
Le point intÃ©ressant ici : l'agent prend en entrÃ©e le `feedback` du critique.
Lors de la premiÃ¨re itÃ©ration, ce feedback sera vide, mais lors des itÃ©rations suivantes, il contiendra les remarques du critique pour amÃ©liorer le prompt.

> Notez aussi que le retour est typÃ© `SdxlPrompts` : LangChain4j se charge de parser la rÃ©ponse JSON du LLM pour en faire un objet Java.

## ğŸï¸ ImageGenerator : le gÃ©nÃ©rateur d'images

Cet agent est un peu particulier car ce n'est **pas un agent basÃ© sur un LLM**.
Il utilise tout de mÃªme un modÃ¨le de gÃ©nÃ©ration d'images (Stable Diffusion XL).

> Je n'utilise pas LangChain4j car StableDiffusion n'est pas intÃ©grÃ© dans la liste des modÃ¨les supportÃ©s.
> Bien que j'aurais pu utiliser la compatibilitÃ© OpenAI, il me manquerait la partie "negative prompt" qui est essentielle pour la qualitÃ© de l'image gÃ©nÃ©rÃ©e.


{|
```java
public class ImageGenerator {

  @Agent(value = "Agent to create an image with Stable Diffusion XL given a prompt and a negative prompt.", outputKey = "imageBase64")
  public ImageContent generateImage(@V("sdxlPrompts") SdxlPrompts sdxlPrompts) throws IOException, InterruptedException {
    IO.println("ğŸï¸ Generating image with SDXL prompts...");
    HttpRequest httpRequest = HttpRequest.newBuilder()
        .uri(URI.create(System.getenv("OVH_AI_ENDPOINTS_SD_URL")))
        .POST(HttpRequest.BodyPublishers.ofString("""
                        {"prompt": "%s", 
                         "negative_prompt": "%s"}
                        """.formatted(sdxlPrompts.prompt, sdxlPrompts.negativePrompt)))
        .header("accept", "application/octet-stream")
        .header("Content-Type", "application/json")
        .header("Authorization", "Bearer " + System.getenv("OVH_AI_ENDPOINTS_ACCESS_TOKEN"))
        .build();

    HttpResponse<byte[]> response = HttpClient.newHttpClient()
        .send(httpRequest, HttpResponse.BodyHandlers.ofByteArray());

    Files.write(Path.of("generated-image.jpeg"), response.body());
    return ImageContent.from(Base64.getEncoder().encodeToString(response.body()), "image/jpeg");
  }
}
```
|}

Vous l'avez vu, c'est une **classe** et non pas une interface.
L'annotation `@Agent` est posÃ©e directement sur la mÃ©thode, et LangChain4j comprend qu'il s'agit d'un agent "programmatique".
Il rÃ©cupÃ¨re le `SdxlPrompts` du contexte agentique (via `@V("sdxlPrompts")`), appelle l'API Stable Diffusion XL, sauvegarde l'image localement, et renvoie le rÃ©sultat en base64 via un objet `ImageContent`.

## ğŸ§‘â€âš–ï¸ VisionCritic : le critique d'art

Le dernier agent est celui qui va permettre de dÃ©terminer si l'image gÃ©nÃ©rÃ©e correspond bien Ã  la demande de l'utilisateur.
Il utilise un **modÃ¨le de vision** (VLLM) pour analyser l'image gÃ©nÃ©rÃ©e et dÃ©terminer si elle correspond bien Ã  la demande de l'utilisateur.

> Dans mon cas j'utilise [Qwen2.5-VL-72B-Instruct](https://www.ovhcloud.com/en/public-cloud/ai-endpoints/catalog/qwen-2-5-vl-72b-instruct/) d'AI Endpoints.

Comme prÃ©cÃ©demment, afin de typer la rÃ©ponse du critique, on dÃ©finit un record `Critique` qui contient un score et un feedback textuel :

{|
```java
public record Critique(double score, String feedback) {
}
```
|}

Et l'agent critique :

{|
```java
public interface VisionCritic {
  @SystemMessage("""
                You are an expert image critic with deep knowledge of visual composition, aesthetics, and prompt adherence.
                You will receive a base64-encoded image and the original user request.
                Analyze how well the generated image matches the user's request.
                Respond with ONLY a JSON object (no markdown, no code fences) in this exact format (value are examples, not fixed):
                {"score": "", "feedback": ""}
                The score must be between 0.0 (terrible match) and 1.0 (perfect match).
                Be constructive in your feedback - explain what should be improved for the next iteration.
                """)
  @Agent(description = "Critiques a generated image against the original user request and provides a score and feedback", outputKey = "critique")
  @UserMessage("""
                Original user request: "{{userRequest}}"
                Please critique this image and provide a score and feedback.
                """)
  Critique critique(@V("userRequest") String userRequest, @UserMessage("{{imageBase64}}") ImageContent imageBase64);
}
```
|}

Le `@UserMessage("{{imageBase64}}")` sur le paramÃ¨tre `ImageContent` permet de passer l'image gÃ©nÃ©rÃ©e directement au modÃ¨le de vision pour qu'il l'analyse.
Le critique va ensuite noter l'image et fournir un feedback qui sera, si nÃ©cessaire, rÃ©injectÃ© dans le `PromptRefiner` pour la prochaine itÃ©ration.

# ğŸ” La boucle ReAct : le loop builder

Maintenant qu'on a nos trois agents, il faut les assembler dans une boucle.
C'est lÃ  qu'intervient le [loop workflow](https://docs.langchain4j.dev/tutorials/agents#loop-workflow) de LangChain4j.

## ğŸ—ï¸ Construction des agents

Avant de construire la boucle, il faut instancier nos agents avec `AgenticServices.agentBuilder()` :

{|
```java
PromptRefiner promptRefiner =  AgenticServices.agentBuilder(PromptRefiner.class)
    .chatModel(chatModel)
    .listener(new AgentListener() {
      @Override
      public void beforeAgentInvocation(AgentRequest request) {
        IO.println("ğŸ“ Invoking promptRefiner");
      }
    })
    .outputKey("sdxlPrompts")
    .build();

VisionCritic visionCritic = AgenticServices.agentBuilder(VisionCritic.class)
    .chatModel(visionModel)
    .listener(new AgentListener() {
      @Override
      public void beforeAgentInvocation(AgentRequest request) {
        IO.println("ğŸ§‘â€âš–ï¸ Invoking visionCritic");
      }
    })
    .outputKey("critique")
    .build();
```
|}

Chaque agent est construit avec son propre modÃ¨le (chat ou vision), un listener pour le debug, et son `outputKey` pour le contexte agentique.

> â„¹ï¸ Notez que le `ImageGenerator` n'a pas besoin de builder puisqu'il n'utilise pas de LLM, un simple `new ImageGenerator()` suffit.

## ğŸ”„ La boucle

Et voici le coeur du sujet, la construction de la boucle ReAct :

{|
```java
UntypedAgent agent = AgenticServices.loopBuilder()
    .maxIterations(3)
    .subAgents(promptRefiner, new ImageGenerator(), visionCritic)
    .testExitAtLoopEnd(true)
    .exitCondition((scope, loopCounter) -> {
      Critique critique = (Critique) scope.readState("critique");
      if (critique == null)
        return false;
      try {
        IO.println("ğŸ§‘â€âš–ï¸ Critic score: %s".formatted(critique.score));
        IO.println("ğŸ“Š Feedback: %s".formatted(critique.feedback));

        scope.writeState("feedback", critique.feedback);

        return critique.score >= 0.8;
      } catch (Exception e) {
        IO.println("ğŸ’¥ Could not parse critic score, continuing loop ğŸ’¥");
        return false;
      }
    })
    .build();
```
|}

Voyons de plus prÃ¨s ce qui se passe :

- **ğŸ”„ `maxIterations(3)`** : on limite Ã  3 itÃ©rations maximum. C'est une sÃ©curitÃ© pour ne pas boucler indÃ©finiment (et ne pas exploser votre consommation de tokens ğŸ’¸)
- **ğŸ‘® `subAgents(promptRefiner, new ImageGenerator(), visionCritic)`** : l'ordre est important ! Ã€ chaque itÃ©ration, les agents sont appelÃ©s dans cet ordre : refiner â¡ï¸ generator â¡ï¸ critic
- **ğŸ›‘ `testExitAtLoopEnd(true)`** : la condition de sortie est Ã©valuÃ©e Ã  la fin de chaque itÃ©ration (aprÃ¨s que les trois agents ont fait leur travail, sinon c'est effectuÃ© aprÃ¨s chaque exÃ©cution d'agent)
- **ğŸ” `exitCondition(...)`** : on rÃ©cupÃ¨re la critique du contexte agentique via `scope.readState("critique")`, on vÃ©rifie le score, et s'il est >= 0.8, on sort de la boucle. Sinon, on Ã©crit le feedback dans le contexte (`scope.writeState("feedback", ...)`) pour que le `PromptRefiner` puisse l'utiliser Ã  l'itÃ©ration suivante

## ğŸš€ L'exÃ©cution

{|
```java
Object result = agent.invoke(Map.of(
    "userRequest", userRequest,
    "feedback", "No previous feedback - this is the first iteration.",
    "imageBase64", ""));
```
|}

On initialise le contexte avec la demande utilisateur, un feedback vide pour la premiÃ¨re itÃ©ration, et une image vide.

# ğŸ¤— En conclusion

Et voilÃ  pour la partie ReAct de votre dÃ©veloppement agentique.
Ã€ vous de voir si vous prÃ©fÃ©rez le mode supervisor comme dÃ©crit dans l'article prÃ©cÃ©dent ou cette approche.
Dans les deux cas, vous allez donner de l'autonomie Ã  votre agent pour itÃ©rer sur son travail et maximiser la qualitÃ© de ses rÃ©ponses.
Attention donc Ã  la consommation de vos tokens ğŸ˜‰.

Le code complet est disponible sous forme de gist [ici](https://gist.github.com/philippart-s/fdfbddbfb20bd795563dadad66315f05).

Si vous Ãªtes arrivÃ©.es jusque-lÃ , merci de m'avoir lu et s'il y a des coquilles n'hÃ©sitez pas Ã  me faire une [issue ou PR](https://github.com/philippart-s/blog) ğŸ˜Š.
