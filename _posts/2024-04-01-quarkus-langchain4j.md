---
title: "âš¡ï¸ Quand Quarkus rencontre LangChain4j ğŸ¦œ"
classes: wide
excerpt: "DÃ©couverte de comment utiliser LangChain4j avec lâ€™extension Quarkus quarkus-langchain4j"
categories:
  - Code
  - Java
  - IA
  
tags:
  - Quarkus
  - Langchain4J
  - IA

---
**TODO** image d'un perroquet qui fait de l'IA
<br/>

Dans le cadre de mon travail, cela fait maintenant plus de deux ans que je navigue dans le monde de l'Intelligence Artificielle.
Et, comme tout le monde, j'ai assistÃ© Ã  la dÃ©ferlante des [Large Languages Model](https://fr.wikipedia.org/wiki/Grand_modÃ¨le_de_langage){:target="_blank"} (LLM).
Depuis quelques mois, le monde de l'IA n'est plus rÃ©servÃ© aux spÃ©cialistes mais accessibles aux dÃ©veloppeuses et dÃ©veloppeurs.
Et comme Ã  chaque fois qu'une nouvelle tendance entre dans notre monde, cela se fait avec beaucoup dâ€™enthousiasme et de passion !
Vous n'Ãªtes, certainement, pas sans savoir que la communautÃ© de dÃ©veloppement s'est passionnÃ© pour un domaine en particulier : le [Retrieval Augmented Generation](https://huggingface.co/docs/transformers/model_doc/rag){:target="_blank"} (RAG).
Pour faire simple : comment spÃ©cialiser un LLM avec vos donnÃ©es Ã  vous.

Comme souvent, des frameworks voient le jour, certains meurs, d'autres sont massivement utilisÃ©s par l'Ã©cosystÃ¨me.
C'est le cas de [LangChain](https://github.com/langchain-ai){:target="_blank"} qui simplifie grandement l'utilisation de LLM et plus particuliÃ¨rement de la partie RAG.

Oui mais voilÃ  sorti de Python et Javascript, langchain n'est pas utilisable.
Vous me voyez venir, moi je fais du Java, et donc naturellement, comme d'habitude, je suis parti Ã  la recherche d'un Ã©quivalent en Java ğŸ˜‰.
Je n'ai pas eu Ã  chercher bien longtemps, trÃ¨s vite un framework a vu le jour, [LangChain4j](https://github.com/langchain4j/langchain4j/){:target="_blank"}.

> âš ï¸ le nom peut Ãªtre trompeur, le projet ne fait pas partie de la galaxy LangChain. C'est un projet Ã  part, avec sa propre communautÃ© et son propre cycle de vie. âš ï¸

Et comme toujours, parce que [Quarkus](https://quarkus.io/){:target="_blank"} c'est cool, j'ai lÃ  aussi eu le plaisir de voir qu'une extension a vite vue le jour : [quarkus-langchain4j](https://github.com/quarkiverse/quarkus-langchain4j){:target="_blank"} ğŸ¤©.

Le dÃ©cor est posÃ©, je vous propose de souter dans le terrier avec moi et de voir comment faire de l'IA, plus particuliÃ¨rement utiliser un LLM, avec LangChain4j au travers de Quarkus ğŸ‡ !

**TODO** image terrier alice lapin et perroquet


## ğŸ§  Les modÃ¨les "compatibles"

La plus part des modÃ¨les possÃ¨dent des API, ce qui les rends _compatibles_ puisqu'il suffit de [coder un client REST](https://quarkus.io/guides/rest-client){:target="_blank"} pour les utiliser.
Cependant, les ressources JSON, les endpoints et tous le reste peut Ãªtre vite rÃ©barbatif.
Et je ne parle pas de la partie RAG, c'est lÃ  oÃ¹ les frameworks vont vous faciliter la vie.

Pour notre premiÃ¨re fois avec LangChain4j je vous propose de commencer simple : dÃ©velopper un chat bot intelligent.
D'autres blog posts suivront par la suite pour aller dans des cas d'usages plus complexes.

SÃ©lectionnons donc le modÃ¨le que l'on veut utiliser, dans notre cas ce sera [Ollama](https://ollama.com/){:target="_blank"}, ce n'est pas Ã  proprement parlÃ© un modÃ¨le mais une faÃ§on de faire tourner des LLM en local.
Pourquoi me direz vous alors que la plupart des acteurs du marchÃ© proposent une API ?
Tout simplement que ces API sont facturÃ©es Ã  l'appel ğŸ˜‰, oui, et cela n'a rien de choquant, il faut bien, Ã  un moment donnÃ©e, que les sociÃ©tÃ© gagnent de l'argent.
Mais ce qui est bien c'est que nombreuses entre elles fournissent leurs modÃ¨les en open source.
Libre Ã  vous de les utiliser tant est que vous soyez en capacitÃ© de les dÃ©ployer et exÃ©cuter.
C'est lÃ  oÃ¹ Ollama nous sauve la mise en facilitant grandement la rÃ©cupÃ©ration et la mise Ã  disposition d'un modÃ¨le sous forme d'API.

Je vous laisse aller voir la documentation d'Ollama, mais au final cela se rÃ©sume Ã  [installer une CLI](https://ollama.com/download){:target="_blank"} puis de [choisir le modÃ¨le](https://ollama.com/library){:target="_blank"} que vous souhaitez utiliser ğŸ˜.

```bash
$ ollama --version
ollama version is 0.1.30
```

Dans mon cas je vais utiliser un modÃ¨le franÃ§ais ... Mistral 7B ğŸ˜Š.

```bash
ollama run mistral
pulling manifest 
pulling e8a35b5937a5... 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– 4.1 GB                         
pulling 43070e2d4e53... 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  11 KB                         
pulling e6836092461f... 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   42 B                         
pulling ed11eda7790d... 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   30 B                         
pulling f9b1e3196ecf... 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  483 B                         
verifying sha256 digest 
writing manifest 
removing any unused layers 
success 

>>> What is Quarkus?
 Quarkus is an open-source, lightweight and extensible Java framework for building modern applications. It was developed by Red Hat as part of the Eclipse 
Foundation and is designed to be reactive, which means it's optimized for building event-driven, non-blocking applications that can handle multiple 
requests concurrently. Quarkus uses a modular design, allowing developers to easily add only the features they need for their project. It also includes 
built-in support for popular frameworks and tools such as RESTEasy, Micronaut, and Hibernate ORM, among others. The goal of Quarkus is to provide a fast 
and efficient way to build modern applications using Java without the need for a full-sized application server like WildFly or Jetty.

>>> Send a message (/? for help)

```

Oui, en dehors des aspects de puissance de calcul, prÃ©voyez de la RAM et de l'espace disque ğŸ˜….
Vous le voyez ici, le CLI permet d'avoir un prompt interactif.
Ce qui mâ€™intÃ©resse c'est de pouvoir faire des call API, Ollama expose aussi un endpoint.

```bash
$ curl -X POST http://localhost:11434/api/generate -d '{
  "model": "mistral",
  "prompt":"Who are you?"                             
 }'

{"model":"mistral","created_at":"2024-04-01T12:34:58.686919Z","response":" I","done":false}
{"model":"mistral","created_at":"2024-04-01T12:34:58.762837Z","response":"'","done":false}
{"model":"mistral","created_at":"2024-04-01T12:34:58.838664Z","response":"m","done":false}
{"model":"mistral","created_at":"2024-04-01T12:34:58.913749Z","response":" an","done":false}
{"model":"mistral","created_at":"2024-04-01T12:34:58.989101Z","response":" artificial","done":false}
{"model":"mistral","created_at":"2024-04-01T12:34:59.065097Z","response":" intelligence","done":false}
{"model":"mistral","created_at":"2024-04-01T12:34:59.141769Z","response":" designed","done":false}
{"model":"mistral","created_at":"2024-04-01T12:34:59.219096Z","response":" to","done":false}
{"model":"mistral","created_at":"2024-04-01T12:34:59.294207Z","response":" assist","done":false}
{"model":"mistral","created_at":"2024-04-01T12:34:59.370426Z","response":" and","done":false}
{"model":"mistral","created_at":"2024-04-01T12:34:59.445633Z","response":" interact","done":false}
{"model":"mistral","created_at":"2024-04-01T12:34:59.521867Z","response":" with","done":false}
{"model":"mistral","created_at":"2024-04-01T12:34:59.597095Z","response":" people","done":false}
{"model":"mistral","created_at":"2024-04-01T12:34:59.673407Z","response":".","done":false}
{"model":"mistral","created_at":"2024-04-01T12:34:59.748168Z","response":" I","done":false}
{"model":"mistral","created_at":"2024-04-01T12:34:59.823759Z","response":" don","done":false}
{"model":"mistral","created_at":"2024-04-01T12:34:59.899905Z","response":"'","done":false}
{"model":"mistral","created_at":"2024-04-01T12:34:59.975477Z","response":"t","done":false}
{"model":"mistral","created_at":"2024-04-01T12:35:00.052783Z","response":" have","done":false}
{"model":"mistral","created_at":"2024-04-01T12:35:00.128817Z","response":" a","done":false}
{"model":"mistral","created_at":"2024-04-01T12:35:00.205785Z","response":" physical","done":false}
{"model":"mistral","created_at":"2024-04-01T12:35:00.282504Z","response":" form","done":false}
{"model":"mistral","created_at":"2024-04-01T12:35:00.359132Z","response":" or","done":false}
{"model":"mistral","created_at":"2024-04-01T12:35:00.43444Z","response":" personal","done":false}
{"model":"mistral","created_at":"2024-04-01T12:35:00.511284Z","response":" identity","done":false}
{"model":"mistral","created_at":"2024-04-01T12:35:00.588228Z","response":",","done":false}
{"model":"mistral","created_at":"2024-04-01T12:35:00.663553Z","response":" but","done":false}
{"model":"mistral","created_at":"2024-04-01T12:35:00.742887Z","response":" I","done":false}
{"model":"mistral","created_at":"2024-04-01T12:35:00.819677Z","response":" can","done":false}
{"model":"mistral","created_at":"2024-04-01T12:35:00.896311Z","response":" process","done":false}
{"model":"mistral","created_at":"2024-04-01T12:35:00.973162Z","response":" information","done":false}
{"model":"mistral","created_at":"2024-04-01T12:35:01.04941Z","response":" and","done":false}
{"model":"mistral","created_at":"2024-04-01T12:35:01.124975Z","response":" communicate","done":false}
{"model":"mistral","created_at":"2024-04-01T12:35:01.201619Z","response":" in","done":false}
{"model":"mistral","created_at":"2024-04-01T12:35:01.279942Z","response":" natural","done":false}
{"model":"mistral","created_at":"2024-04-01T12:35:01.358783Z","response":" language","done":false}
{"model":"mistral","created_at":"2024-04-01T12:35:01.435508Z","response":".","done":false}
{"model":"mistral","created_at":"2024-04-01T12:35:01.51132Z","response":" My","done":false}
{"model":"mistral","created_at":"2024-04-01T12:35:01.588259Z","response":" main","done":false}
{"model":"mistral","created_at":"2024-04-01T12:35:01.664898Z","response":" goal","done":false}
{"model":"mistral","created_at":"2024-04-01T12:35:01.742348Z","response":" is","done":false}
{"model":"mistral","created_at":"2024-04-01T12:35:01.818447Z","response":" to","done":false}
{"model":"mistral","created_at":"2024-04-01T12:35:01.894563Z","response":" help","done":false}
{"model":"mistral","created_at":"2024-04-01T12:35:01.971313Z","response":" answer","done":false}
{"model":"mistral","created_at":"2024-04-01T12:35:02.048371Z","response":" questions","done":false}
{"model":"mistral","created_at":"2024-04-01T12:35:02.123999Z","response":",","done":false}
{"model":"mistral","created_at":"2024-04-01T12:35:02.200502Z","response":" provide","done":false}
{"model":"mistral","created_at":"2024-04-01T12:35:02.277574Z","response":" information","done":false}
{"model":"mistral","created_at":"2024-04-01T12:35:02.354164Z","response":",","done":false}
{"model":"mistral","created_at":"2024-04-01T12:35:02.42975Z","response":" and","done":false}
{"model":"mistral","created_at":"2024-04-01T12:35:02.505634Z","response":" engage","done":false}
{"model":"mistral","created_at":"2024-04-01T12:35:02.581927Z","response":" in","done":false}
{"model":"mistral","created_at":"2024-04-01T12:35:02.657807Z","response":" conversation","done":false}
{"model":"mistral","created_at":"2024-04-01T12:35:02.735058Z","response":" on","done":false}
{"model":"mistral","created_at":"2024-04-01T12:35:02.813505Z","response":" various","done":false}
{"model":"mistral","created_at":"2024-04-01T12:35:02.891128Z","response":" topics","done":false}
{"model":"mistral","created_at":"2024-04-01T12:35:02.967898Z","response":".","done":false}
{"model":"mistral","created_at":"2024-04-01T12:35:03.043847Z","response":" How","done":false}
{"model":"mistral","created_at":"2024-04-01T12:35:03.120204Z","response":" may","done":false}
{"model":"mistral","created_at":"2024-04-01T12:35:03.198188Z","response":" I","done":false}
{"model":"mistral","created_at":"2024-04-01T12:35:03.274229Z","response":" assist","done":false}
{"model":"mistral","created_at":"2024-04-01T12:35:03.350503Z","response":" you","done":false}
{"model":"mistral","created_at":"2024-04-01T12:35:03.42655Z","response":" today","done":false}
{"model":"mistral","created_at":"2024-04-01T12:35:03.503526Z","response":"?","done":false}
{"model":"mistral","created_at":"2024-04-01T12:35:03.580365Z","response":"","done":true,"context":[733,16289,28793,28705,6526,460,368,28804,733,28748,16289,28793,315,28742,28719,396,18278,10895,5682,298,6031,304,14113,395,905,28723,315,949,28742,28707,506,264,5277,1221,442,3327,8208,28725,562,315,541,1759,1871,304,16287,297,4229,3842,28723,1984,2191,5541,349,298,1316,4372,4224,28725,3084,1871,28725,304,14200,297,7114,356,4118,13817,28723,1602,993,315,6031,368,3154,28804],"total_duration":5391755791,"load_duration":4218041,"prompt_eval_count":8,"prompt_eval_duration":493057000,"eval_count":65,"eval_duration":4892809000}
```

Ca c'est fait, nous avons notre LLM Ã  disposition pour nos tests, passons aux choses agrÃ©able : coder !

## âš¡ï¸ + ğŸ¦œ

CrÃ©ons notre projet Quarkus en activant l'extension quarkus-langchain4j-ollama : `quarkus create app fr.wilda.quarkus:discover-langchain4j --extension='quarkus-langchain4j-ollama'`.
A ce stade, on reste dans du classique avec Quarkus : le projet est initialisÃ© avec tout ce qui va bien en termes d'arborescences et de configuration de dÃ©pendances dans le pom.xml.

Pour faire notre chat bot, la premiÃ¨re chose que je vous conseille est d'aller voir la documentation de l'extension.
Elle liste les diffÃ©rents modÃ¨les supportÃ©s ainsi que les configurations Ã  activer.
Cela permet aussi d'avoir quelques exemples de code.

> âš ï¸ Il se peut que le code pour utiliser un modÃ¨le soit prÃ©sent et pas le documentation. C'Ã©tait le cas au moment oÃ¹ j'Ã©cris cet article pour Mistral par exemple. N'hÃ©sitez pas Ã  jeter un oeil directement au code pour vous en assurer ğŸ˜‰.

### ğŸ¤– Le service 

Le [service](https://docs.quarkiverse.io/quarkus-langchain4j/dev/ai-services.html){:target="_blank"} est la base de votre code pour interagir avec le modÃ¨le.
Il va vous permettre de dÃ©finir un contexte mais aussi de paramÃ©trer vos interactions avec le modÃ¨le par le biais de variables.

```java
@RegisterAiService
public interface OllamaAIService {
  
  @SystemMessage("You are an AI assistant.")  
  @UserMessage("Answer as best possible to the following question: {question}. The answer must be in a style of a virtual assistant ans use emoji.")
  String askAQuestion(String question);
}
```
Ici on donne un peu de contexte Ã  notre LLM afin qu'il nous rÃ©ponde dans le style que l'on souhaite.
On peut le faire via le [system message](https://docs.quarkiverse.io/quarkus-langchain4j/dev/ai-services.html#_system_message){:target="_blank"} et le [user message](https://docs.quarkiverse.io/quarkus-langchain4j/dev/ai-services.html#_user_message_prompt){:target="_blank"}.

A cela il faut ajouter quelques Ã©lÃ©ments de configuration (portionnables aussi en variables dâ€™environnement ou via programmation).
```java
quarkus.langchain4j.ollama.chat-model.enabled=true
quarkus.langchain4j.ollama.log-requests=true
quarkus.langchain4j.ollama.timeout=60s
quarkus.langchain4j.ollama.embedding-model.enabled=false
quarkus.langchain4j.ollama.base-url=http://localhost:11434/
quarkus.langchain4j.ollama.chat-model.model-id=mistral
```

### ğŸ¤– La classe principale




# En conclusion


Si vous Ãªtes arrivÃ©s jusque lÃ  merci de m'avoir lu et si il y a des coquilles n'hÃ©sitez pas Ã  me faire une [issue ou PR](https://github.com/philippart-s/blog){:target="_blank"} ğŸ˜Š.