---
title: "ü§ñ Augmente les capacit√©s de ton IA avec LangChain4j ü¶ú"
classes: wide
excerpt: "Deuxi√®me partie de la d√©couverte de LangChain4j au travers de Quarkus."
categories:
  - Code
  - Java
  - IA
  
tags:
  - Quarkus
  - LangChain4J
  - IA

---
![perroquet multi couleurs avec un robot]({{ site.url }}{{ site.baseurl }}/assets/images/quarkus-langchain4j-streaming/robot-ia.jpg){: .align-center}
[@wildagsx](https://twitter.com/wildagsx){:style="font-size: smaller"}{:target="_blank"}{: .align-right}<br/>

Je vous propose la suite de l'[article pr√©c√©dent]({{ site.baseurl }}{% post_url 2024-04-01-quarkus-langchain4j %}){:target="_blank"} nous ayant permis la d√©couverte de [LangChain4j](https://docs.langchain4j.dev/intro/){:target="_blank"} au travers de [Quarkus](https://quarkus.io/){:target="_blank"}.
> ‚ÑπÔ∏è Je vous laisse donc y jeter un oeil pour toute la phase d'installation, de pr√©requis n√©cessaires √† la bonne compr√©hension de cet article.

Lors de ce premier article, nous avons vu ensemble comment d√©velopper notre premier chat bot.
Celui-ci √©tait tr√®s simple et il fallait attendre que le mod√®le distant _fabrique_ l'ensemble de la r√©ponse avant de l'avoir en retour.
Pas tr√®s pratique et convivial.

Cette fois-ci, je vous propose d'ajouter quelques fonctionnalit√©s rendant notre _chat bot_ plus "intelligent".  
On va donc le faire se comporter comme un _chat bot_ normal : streamer sa r√©ponse.
On va aussi lui ajouter un peu de contexte afin qu'il connaisse plus de choses essentielles üòâ !


>‚ÑπÔ∏è L'ensemble du code source se trouve dans le repository GitHub [discover-langchain4j](https://github.com/philippart-s/discover-langchain4j){:target="_blank"}

## üåä Activation du mode streaming

C'est la premi√®re fonctionnalit√© que nous allons rajouter : cela permet de rendre le bot plus convivial et de ne pas avoir √† attendre sans trop savoir quand il va nous r√©pondre üòÖ.  
Pour activer cette fonctionnalit√©, c'est assez simple : nous allons utiliser [Mutiny](https://quarkus.io/guides/mutiny-primer){:target="_blank"}.
Mais c'est quoi me direz-vous ü§® ?  
En deux mots : cela vous permet d'ajouter une notion d'asynchronisme dans votre d√©veloppement et de basculer dans ce que l'on appelle la programmation reactive.  

L'objectif ?
Permettre √† notre IA d'envoyer son d√©but de r√©ponse avant m√™me d'avoir envoy√© l'ensemble de la r√©ponse.

### üîÄ Ajout de l'asynchronisme

[Quarkus](https://quarkus.io/){:target="_blank"} et son extension [quarkus-langchain4j](https://github.com/quarkiverse/quarkus-langchain4j/){:target="_blank"} vont encore grandement nous aider.

> ‚ÑπÔ∏è Nous repartons du code de l'[article pr√©c√©dent]({{ site.baseurl }}{% post_url 2024-04-01-quarkus-langchain4j %}){:target="_blank"}, si vous souhaitez plus de d√©tails n'h√©sitez pas √† vous reporter √† l'article üòâ.

Nous allons donc modifier notre service _OllamaService.java_ pour qu'il supporte le mode streaming : 

```java
package fr.wilda.quarkus;

import dev.langchain4j.service.SystemMessage;
import dev.langchain4j.service.UserMessage;
import io.quarkiverse.langchain4j.RegisterAiService;
import io.smallrye.mutiny.Multi;

// AI service bean registration
@RegisterAiService
public interface OllamaAIService {
  
  // Context message
  @SystemMessage("You are an AI assistant.")  
  // Prompt customisation
  @UserMessage("Answer as best possible to the following question: {question}. The answer must be in a style of a virtual assistant and use emoji.")
  String askAQuestion(String question);

  // Context message
  @SystemMessage("You are an AI assistant.")  
  // Prompt customisation
  @UserMessage("Answer as best possible to the following question: {question}. The answer must be in a style of a virtual assistant and use emoji.")
  // Multi use is enough to activate streaming mode
  Multi<String> askAQuestionStreamingMode(String question);

}
```

‚ö†Ô∏è Notez-bien ici l'utilisation de l'interface `io.smallrye.mutiny.Multi` qui permet "d'activer" le mode streaming.
L'extension se charge de l'activer lors de ses requ√™tes au mod√®le üòâ.  
A noter que seul le type `String` est, pour l'instant, support√© pour le mode streaming mais des √©tudes d'√©volutions sont en cours ‚ö†Ô∏è

Maintenant, nous allons faire √©voluer notre partie API pour qu'elle puisse profiter de cette arriv√©e d'informations au fil de l'eau.

### üß© Modification de l'API Rest

L'id√©e est de proposer une ressource qui va √™tre _stream√©e_ au fur et √† mesure.
On met donc √† jour la  classe `AIAssistant` qui expose le endpoint `hal9000`.

```java
package fr.wilda.quarkus;

import org.jboss.resteasy.reactive.RestQuery;
import io.smallrye.mutiny.Multi;
import jakarta.inject.Inject;
import jakarta.ws.rs.GET;
import jakarta.ws.rs.POST;
import jakarta.ws.rs.Path;

// Endpoint root path
@Path("hal9000")
public class AIAssistant {

  // AI Service injection to use it later
  @Inject
  OllamaAIService ollamaAIService;

  // ask resource exposition with POST method
  @Path("ask")
  @POST
  public String ask(String question) {
    // Call the Mistral AI chat model
    return ollamaAIService.askAQuestion(question);
  }

  // Stream response
  @Path("streaming")
  @GET
  public Multi<String> streaming(@RestQuery("question") String question) {
    // Call the Mistral AI chat model
    return ollamaAIService.askAQuestionStreamingMode(question);
  }
}
```

On a rajout√© une ressource `streaming` qui retourne `Multi<String>`, Quarkus fera le reste.  
Ensuite il suffit d'aller sur l'URL `http://localhost:8080/hal9000/streaming?question="What is the answer to the Ultimate Question of Life,the Universe, and Everything?"`

{% include video id="T0LbsThvaRY" provider="youtube" %}

## üëå Ajoutons un peu de contexte

L'intelligence des diff√©rents mod√®les que l'on utilise d√©pend grandement des donn√©es utilis√©es mais aussi de quand date la derni√®re _indexation_ de ces donn√©es (oui je sais que ce n'est pas le terme standard mais √ßa permet de se comprendre üòâ).

Essayons ce prompt, voulez-vous : _Can you tell me more about St√©phane Philippart?_  

Voil√† la r√©ponse donn√©e:  
```
üëã Hello! St√©phane Philippart is a renowned figure in the tech industry, particularly in Belgium.
üáßüá™üíª He's known for his expertise in Information Technology (IT) and Digital Transformation. St√©phane has spent over two decades in the tech sector, holding various key positions in leading companies.
üïí‚ú® His achievements include co-founding several successful startups and contributing significantly to their growth. 
He's also a sought-after speaker at industry events, sharing his insights on digital transformation trends._
```

Flatteur mais tr√®s loin de la r√©alit√© non ü§® ?

### üóÉÔ∏è Le RAG √† la rescousse

Rien d'√©tonnant dans cette r√©ponse :
 - je ne suis pas tr√®s connu et donc pas √©tonnant de pas avoir beaucoup de donn√©es sur moi
 - mon pr√©nom, assez commun, fait que l'hallucination avec d'autres _St√©phane_ n'est pas √©tonnante
 - la plupart des √©l√©ments publiques datent de 2-3 ans et souvent les mod√®les ont √©t√© entra√Æn√©s sur des donn√©es plus anciennes

En quoi le RAG va nous aider ?  
Le RAG ou encore _Retrieval Augmented Generation_ va vous permettre d'ajouter des donn√©es non connues de votre mod√®le pour lui donner un contexte qui correspond √† votre domaine fonctionnel.
Pour ajouter ce contexte on va prendre une source de donn√©es, par exemple des fichiers, puis les transformer dans un format permettant une recherche par similitudes, dans ce cas une base de donn√©es vectorielle.  

Une fois ce contenu ajout√©, lors de requ√™tes envoy√©es au mod√®le, celui-ci va pouvoir se baser sur ces donn√©es suppl√©mentaires pour contextualiser sa r√©ponse.

> Le RAG se diff√©rencie d'une autre technique, le transfert learning, par le fait que l'on utilise le mod√®le tel quel en lui ajoutant des donn√©es / du contexte.  
> Le transfert learning va consister √† r√©-entrainer un mod√®le pour un use case diff√©rent de celui d‚Äôorigine, cela demande donc plus de calculs et une phase d‚Äôentra√Ænement l√† o√π le RAG se fait au runtime.

### üìÉ Les donn√©es √† rajouter

Je vous l'ai dit, on peut choisir plusieurs types de donn√©es.
Dans mon cas, je vais choisir un fichier texte.

```
St√©phane Philippart is a world-renowned developer advocate in the field of cloud computing. He is also the CTO of Tours' biggest meetup.
Tours is well known for its famous rillettes.
```

### üß© Activation du RAG

Pour utiliser le RAG il va falloir deux choses : 
 - un moyen de transformer nos donn√©es en vecteurs
 - un moyen d'ajouter ces donn√©es dans la cha√Æne d'interrogation de notre mod√®le

L√† encore, vous vous en doutez, Quarkus et LangChain4j vont nous √™tre d'un grand secours !

```xml
<!-- To add RAG capabilities -->
<dependency>
    <groupId>io.quarkiverse.langchain4j</groupId>
    <artifactId>quarkus-langchain4j-easy-rag</artifactId>
    <version>0.13.0</version>
</dependency>  

<!-- Inner process embedding model -->
<dependency>
    <groupId>dev.langchain4j</groupId>
    <artifactId>langchain4j-embeddings-all-minilm-l6-v2-q</artifactId>
    <version>0.30.0</version>
</dependency>
```
Merci √† l'extension Quarkus qui, par le simple ajout de la d√©pendance `quarkus-langchain4j-easy-rag`, active le mode [Easy RAG](https://docs.quarkiverse.io/quarkus-langchain4j/dev/easy-rag.html){:target="_blank"}.

Notez l'ajout de la d√©pendance `dev.langchain4j:langchain4j-embeddings-all-minilm-l6-v2-q` qui me permet d'avoir un [_embedding model_ in process](https://docs.quarkiverse.io/quarkus-langchain4j/dev/in-process-embedding.html){:target="_blank"}.
J'ai choisi cela plut√¥t que d'utiliser le endpoint d'embedding de Mistral car en mode Ollama il n'est pas accessible avec LangChain4j.

Au chargement de l'application on voit que le mode RAG est activ√© avec les bonnes donn√©es : 
```bash
2024-05-05 20:12:23,987 INFO  [io.qua.lan.eas.run.EasyRagRecorder] (Quarkus Main Thread) Ingesting documents from path: ./src/main/resources/rag/, path matcher = glob:**, recursive = true
```

R√©essayons de demander au mod√®le s'il conna√Æt des choses sur _St√©phane Philippart_ !  
L‚Äôappel √† l'URL `http://localhost:8080/hal9000/streaming?question="Can you tell me more about St√©phane Philippart?"` donne cette fois : 
```
üåê Hey there! St√©phane Philippart,üë®‚Äçüíª is a globally recognized developer advocate in the cloud computing domain! *claps* 
His expertise is highly sought after, making him a key figure in this innovative field.üí°Moreover, St√©phane holds an impressive title as the CTO of Tours' biggest meetup!
üè¢ This city in France, famously known for its scrumptious rillettes,ü•ìüòã is where he makes a significant impact`
```

Pas forc√©ment plus vrai mais mieux quand m√™me non ? üòâ

**‚ö†Ô∏è Ce petit exemple doit vous faire allumer quelques alertes dans votre cerveau : on ne peut d√©finitivement pas faire confiance aux r√©sultats d'une IA que l'on peut si facilement biaiser en quelques lignes de codes ‚ö†Ô∏è**

# En conclusion

J'esp√®re que vous avez pu, simplement, vous rendre compte comme il est assez facile avec Quarkus et LangChain4j d'ajouter des capacit√©s √† notre application de _chat bot_.
Je vais continuer √† ajouter quelques autres choses plus ou moins utiles dans les articles suivants üòâ.

Si vous √™tes arriv√©s jusque l√† merci de m'avoir lu et si il y a des coquilles n'h√©sitez pas √† me faire une [issue ou PR](https://github.com/philippart-s/blog){:target="_blank"} üòä.

Merci √† ma relectrice, Fanny, qui vous permet de lire cet article sans avoir trop les yeux qui saignent üòò.

L'ensemble des sources des exemples est disponible dans le repository GitHub [langchain4j-discovery](https://github.com/philippart-s/discover-langchain4j){:target="_blank"}.