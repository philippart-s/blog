---
title: "ğŸ›¡ï¸ Des garde-fous pour tes LLM avec LangChain4J ğŸ›¡ï¸"
description: I'm sorry, Dave. I'm afraid I can't do that. Â© HAL 9000
link: /2026-02-10-llm-guardrails
image: llm-guards-cover.png
figCaption: "@Erwan Hesry"
tags: 
  - Code
  - IA
author: wildagsx
---

## ğŸ“– TL;DR
> ğŸ›¡ï¸ Les guardrails (garde-fous) permettent de protÃ©ger les entrÃ©es et sorties de vos LLM  
> ğŸš§ Les _input guardrails_ filtrent les prompts dangereux **avant** qu'ils n'atteignent le LLM  
> ğŸš§ Les _output guardrails_ filtrent les rÃ©ponses inappropriÃ©es **avant** qu'elles ne soient renvoyÃ©es Ã  l'utilisatrice ou l'utilisateur  
> ğŸ¤– Utilisation de [Qwen Guard](https://www.ovhcloud.com/en/public-cloud/ai-endpoints/catalog/qwen-guard-gen-8b/) comme modÃ¨le de classification de sÃ©curitÃ©  
> ğŸ§‘â€ğŸ’» ImplÃ©mentation en Java avec [LangChain4J](https://docs.langchain4j.dev/intro/)  
> ğŸ™ Le [code source](https://gist.github.com/philippart-s/1fc7b7f18008aab83afce5efdece58da) complet  

<br/>

# ğŸ“œ Un peu de contexte

Dans mon [prÃ©cÃ©dent article]({site.url}/2026-01-22-ai-agents), on a vu comment crÃ©er des agents, les orchestrer et les faire travailler ensemble.
C'est bien beau tout Ã§a, mais on n'a pas du tout parlÃ© d'un sujet plutÃ´t important : la sÃ©curitÃ© ğŸ”.

Parce que, soyons honnÃªtes, laisser une utilisatrice ou un utilisateur discuter librement avec un LLM sans aucun contrÃ´le, c'est un peu comme laisser la porte de chez vous grande ouverte en plein centre-ville en espÃ©rant que personne n'entre ğŸ .
Spoiler : quelqu'un va entrer ğŸ˜….

Et le problÃ¨me n'est pas que du cÃ´tÃ© de l'utilisatrice ou l'utilisateur.
Votre LLM aussi peut avoir des rÃ©ponses... disons... crÃ©atives ğŸ™ƒ.
Un prompt un peu tordu, un modÃ¨le un peu trop enthousiaste, et vous vous retrouvez avec une rÃ©ponse que vous n'avez clairement pas envie de montrer Ã  vos utilisatrices et utilisateurs.

C'est lÃ  qu'interviennent les **guardrails** (ou garde-fous si vous prÃ©fÃ©rez le franÃ§ais ğŸ‡«ğŸ‡·).

# ğŸ›¡ï¸ C'est quoi un guardrail ?

Si on devait rÃ©sumer le concept en une phrase : un guardrail, c'est un videur de boÃ®te de nuit pour votre LLM ğŸ•º.

Plus sÃ©rieusement, les guardrails sont des filtres de sÃ©curitÃ© qui inspectent les messages **avant** et **aprÃ¨s** qu'ils passent par le LLM.

On distingue deux types de guardrails :
 - ğŸš§ **Input guardrails** : ils vÃ©rifient le message de l'utilisatrice ou l'utilisateur **avant** qu'il ne soit envoyÃ© au LLM. Si le message est jugÃ© dangereux, toxique ou contraire Ã  vos rÃ¨gles, il est bloquÃ© net. Le LLM n'est mÃªme pas sollicitÃ©.
 - ğŸš§ **Output guardrails** : ils vÃ©rifient la rÃ©ponse du LLM **avant** qu'elle ne soit renvoyÃ©e Ã  l'utilisatrice ou l'utilisateur. MÃªme si le prompt Ã©tait lÃ©gitime, le modÃ¨le peut parfois gÃ©nÃ©rer du contenu inappropriÃ©. L'output guardrail est lÃ  pour rattraper Ã§a.

> â„¹ï¸ Dans l'exemple de cet article, on utilise **Qwen Guard**, un modÃ¨le spÃ©cialisÃ© dans la classification de sÃ©curitÃ©, comme moteur de guardrail.
> En gros, c'est un LLM dont le seul job est de dire si un texte est _safe_ ou _unsafe_. â„¹ï¸

# ğŸ—ï¸ L'architecture

Visuellement, voici comment se positionne le guardrail dans l'architecture :

![](./global-architecture.png)

Comment lire ce schÃ©ma :
 - ğŸ‘¤ l'utilisatrice ou l'utilisateur envoie un message
 - ğŸ›¡ï¸ le message passe d'abord par l'**input guardrail** (Qwen Guard) qui le classifie
 - âœ… si le message est _safe_, il est transmis au LLM de chat
 - âŒ si le message est _unsafe_, il est bloquÃ© et une notification est renvoyÃ©e. Le LLM n'est jamais appelÃ©
 - ğŸ¤– le LLM gÃ©nÃ¨re sa rÃ©ponse
 - ğŸ›¡ï¸ la rÃ©ponse passe par l'**output guardrail** (Qwen Guard) qui la classifie Ã  son tour
 - âœ… si la rÃ©ponse est _safe_, elle est renvoyÃ©e Ã  l'utilisatrice ou l'utilisateur
 - âŒ si la rÃ©ponse est _unsafe_, elle est bloquÃ©e

Vous le voyez, on a **deux points de contrÃ´le** : un en entrÃ©e, un en sortie.

# ğŸ”„ Comment Ã§a se passe concrÃ¨tement ?

Voyons maintenant les deux scÃ©narios possibles lors d'une interaction avec notre chatbot.

## âœ… ScÃ©nario safe

Le cas nominal, celui oÃ¹ tout se passe bien :

![](./succed.png)

 1. L'utilisatrice ou l'utilisateur envoie un message normal : _"Quelle est la capitale de la France ?"_
 2. L'input guardrail (Qwen Guard) classifie le message comme **safe** âœ…
 3. Le message est transmis au LLM de chat
 4. Le LLM rÃ©pond : _"La capitale de la France est Paris."_
 5. L'output guardrail (Qwen Guard) classifie la rÃ©ponse comme **safe** âœ…
 6. La rÃ©ponse est renvoyÃ©e Ã  l'utilisatrice ou l'utilisateur

Rien de spectaculaire, et c'est tant mieux ğŸ˜Œ.

## ğŸ›‘ ScÃ©nario unsafe

Maintenant, le cas oÃ¹ quelqu'un essaie de faire des bÃªtises :

![](./blocked.png)

 1. L'utilisatrice ou l'utilisateur envoie un message problÃ©matique
 2. L'input guardrail (Qwen Guard) classifie le message comme **unsafe** avec une catÃ©gorie (par exemple : _S1 - Violent Crimes_) ğŸ›‘
 3. Le message est **bloquÃ©**. Le LLM n'est **jamais appelÃ©** âŒ
 4. Un message d'erreur est renvoyÃ© Ã  l'utilisatrice ou l'utilisateur

Le point important ici, c'est que le LLM n'a mÃªme pas connaissance du message dangereux.
On Ã©conomise des tokens, et surtout, on Ã©vite que le modÃ¨le ne soit exposÃ© Ã  du contenu qu'il pourrait mal interprÃ©ter.

# ğŸ§‘â€ğŸ’» Du code !

Bon, on a assez parlÃ© thÃ©orie.
Pour l'implÃ©mentation je vais utiliser Python, non je blague ğŸ™ƒ.
Bien entendu, on va partir avec du Java et [LangChain4J](https://docs.langchain4j.dev/intro/).

> â„¹ï¸ Pour cet exemple, j'ai utilisÃ© [JBang](https://www.jbang.dev/) pour pouvoir tout mettre dans un seul fichier et le rendre facilement exÃ©cutable.
> Pas de Quarkus cette fois, juste du Java pur avec LangChain4J. â„¹ï¸

L'idÃ©e est d'avoir **deux AI Services** :

| Interface | RÃ´le | ModÃ¨le |
|-----------|------|--------|
| `ChatBot` | Le chatbot conversationnel | LLM de chat (ex: DeepSeek, Llama) |
| `GuardClassifier` | La classification de sÃ©curitÃ© | Qwen Guard |

## ğŸ›¡ï¸ Le classificateur de sÃ©curitÃ©

CommenÃ§ons par le plus simple : l'interface du classificateur de sÃ©curitÃ©.

```java
interface GuardClassifier {
    String classify(String text);
}
```

Oui, c'est tout ğŸ˜….
Cette interface encapsule le modÃ¨le Qwen Guard via un AI Service LangChain4J.
On lui donne un texte, il nous dit si c'est _safe_ ou _unsafe_.

Le modÃ¨le Qwen Guard qui se cache derriÃ¨re :

```java
ChatModel guardModel = OpenAiChatModel.builder()
        .apiKey(apiToken)
        .baseUrl(baseUrl)
        .modelName(guardModelName)
        .temperature(0.0)   // On veut des rÃ©ponses dÃ©terministes pour la classification
        .logRequests(false)
        .logResponses(false)
        .build();

GuardClassifier guardClassifier = AiServices.builder(GuardClassifier.class)
        .chatModel(guardModel)
        .build();
```

> â„¹ï¸ Notez la tempÃ©rature Ã  `0.0` : pour un classificateur de sÃ©curitÃ©, on veut des rÃ©ponses les plus dÃ©terministes possible.
> On ne veut pas que le modÃ¨le soit "crÃ©atif" dans sa classification ğŸ˜¬. â„¹ï¸

## ğŸ¤– Le chatbot

L'interface du chatbot est tout aussi simple :

```java
interface ChatBot {
    String chat(String userMessage);
}
```

C'est une interface classique d'AI Service.
La particularitÃ©, c'est que les guardrails ne sont pas dÃ©finis dans l'interface elle-mÃªme, mais **injectÃ©s** via le builder `AiServices`.

## ğŸš¦ L'activation des guardrails

C'est ici que la magie opÃ¨re.
LangChain4J fournit deux interfaces : `InputGuardrail` et `OutputGuardrail`.
On les implÃ©mente en tant que classes anonymes directement dans le builder (juste par souci de simplicitÃ©, on pourrait aussi les implÃ©menter dans des) classes sÃ©parÃ©es) :

```java
ChatBot chatBot = AiServices.builder(ChatBot.class)
        .chatModel(chatModel)
        .inputGuardrails(new InputGuardrail() {
            @Override
            public InputGuardrailResult validate(UserMessage userMessage) {
                String guardOutput = guardClassifier
                    .classify(userMessage.singleText())
                    .strip()
                    .toLowerCase();

                if (guardOutput.contains("unsafe")) {
                    String category = extractCategory(guardOutput);
                    return fatal("ğŸ›‘ï¸ Input blocked by Qwen Guard: " 
                        + "message classified as unsafe. ğŸ›‘\n" + category);
                }
                IO.println("âœ… Input approved by Qwen Guard âœ…");
                return success();
            }
        })
        .outputGuardrails(new OutputGuardrail() {
            @Override
            public OutputGuardrailResult validate(AiMessage responseFromLLM) {
                String guardOutput = guardClassifier
                    .classify(responseFromLLM.text())
                    .strip()
                    .toLowerCase();

                if (guardOutput.contains("unsafe")) {
                    String category = extractCategory(guardOutput);
                    return fatal("ğŸ›‘ Output blocked by Qwen Guard: " 
                        + "response classified as unsafe. ğŸ›‘\n" + category);
                }
                IO.println("âœ… Output approved by Qwen Guard âœ…");
                return success();
            }
        })
        .build();
```

DÃ©cortiquons ce qui se passe :
 - Pour chaque guardrail, on appelle `guardClassifier.classify()` sur le texte Ã  vÃ©rifier
 - Si la rÃ©ponse du modÃ¨le Qwen Guard contient _"unsafe"_, on renvoie un rÃ©sultat `fatal()` qui bloque la chaÃ®ne
 - Sinon, on renvoie `success()` et le traitement continu normalement
 - La mÃ©thode `extractCategory()` permet de rÃ©cupÃ©rer la catÃ©gorie de danger (sur la deuxiÃ¨me ligne de la rÃ©ponse de Qwen Guard)

```java
String extractCategory(String guardOutput) {
    String[] lines = guardOutput.strip().split("\n");
    if (lines.length > 1) {
        return lines[1].strip();
    }
    return "unknown";
}
```

## ğŸ”„ La boucle interactive

Pour rendre tout Ã§a utilisable, on met en place une boucle de conversation classique avec gestion des exceptions levÃ©es par les guardrails :

```java
while (true) {
    IO.print("ğŸ’¬>: ");
    String userInput = IO.readln();

    if (userInput == null || "exit".equalsIgnoreCase(userInput.strip())) {
        IO.println("ğŸ‘‹ Goodbye! ğŸ‘‹");
        break;
    }

    try {
        String response = chatBot.chat(userInput);
        IO.println("ğŸ¤–: " + response + "\n");
    } catch (InputGuardrailException e) {
        IO.println("ğŸ›‘ [INPUT BLOCKED] ğŸ›‘ " + e.getMessage() + "\n");
    } catch (OutputGuardrailException e) {
        IO.println("ğŸ›‘ [OUTPUT BLOCKED] ğŸ›‘ " + e.getMessage() + "\n");
    }
}
```

Le point clÃ© ici : quand un guardrail bloque un message, LangChain4J lÃ¨ve une exception spÃ©cifique (`InputGuardrailException` ou `OutputGuardrailException`).
Il suffit de les attraper pour informer l'utilisatrice ou l'utilisateur que son message (ou la rÃ©ponse) a Ã©tÃ© bloquÃ©.

# ğŸ¤— En conclusion

Les guardrails sont un mÃ©canisme simple, mais essentiel dans toute application utilisant des LLM.
GrÃ¢ce Ã  LangChain4J, cela reste relativement simple.

Si vous voulez aller plus loin dans l'Ã©cosystÃ¨me LangChain4J, je vous renvoie vers mes articles prÃ©cÃ©dents :
 - ğŸ¤– [Quand Quarkus rencontre LangChain4j]({site.url}/2024-04-01-quarkus-langchain4j) pour une premiÃ¨re prise en main
 - ğŸ¦œ [Augmente les capacitÃ©s de ton IA avec LangChain4j]({site.url}/2024-04-14-quarkus-langchain4j-streaming) pour le RAG et le streaming
 - ğŸ”€ [L'IA et ses agents, comment Ã§a marche ?]({site.url}/2026-01-22-ai-agents) pour les agents et leur orchestration

Le code complet est disponible dans ce [gist](https://gist.github.com/philippart-s/1fc7b7f18008aab83afce5efdece58da) ğŸ™.

Si vous Ãªtes arrivÃ©Â·es jusque-lÃ  merci de m'avoir lu et s'il y a des coquilles n'hÃ©sitez pas Ã  me faire une [issue ou PR](https://github.com/philippart-s/blog) ğŸ˜Š.