---
title: "ü§ñ Et si les LLM √©taient le nouveau front ? üé®"
date: 2025-07-13
classes: wide
excerpt: "On nous promet la fin des d√©veloppeuses et d√©veloppeurs, mais si au final l'IA n'√©tait qu'une nouvelle fa√ßon de faire du front ?" 
categories:
  - Blog
  - IA

tags:
  - IA
  - Front
  - LLM
---

![]({{ site.url }}{{ site.baseurl }}/assets/images/ai-as-front/cover.webp){: .align-center}
[@wildagsx](https://twitter.com/wildagsx){:style="font-size: smaller"}{:target="_blank"}{: .align-right}<br/>

L'intelligence artificielle va-t-elle remplacer toute une partie de m√©tiers que l'on conna√Æt actuellement ?
C'est une des th√©ories que l'on entend le plus souvent ces derniers temps.
Et plus particuli√®rement dans notre microcosme, celui du d√©veloppement, n'est-on pas en train de scier la branche sur laquelle on est assis ?

En toute honn√™tet√© je n'en sais rien üòÖ ... tout √ßa pour √ßa me direz-vous !

Ce qui a l'air plut√¥t s√ªr c'est que cela risque de changer la fa√ßon dont nous exer√ßons notre m√©tier mais aussi la mani√®re que les utilisatrices et utilisateurs interagissent avec les applications que nous cr√©ons.
Et c'est l√† que je veux en venir : le front va-t-il dispara√Ætre ?

Derri√®re cette question un brin racoleuse, on pourrait la nuancer : l'interface Homme - Machine va-t-elle se transformer avec l'arriv√©e massive des chatbots boost√©s √† l'IA ?

## üñ•Ô∏è Le dialogue Homme - Machine, une vielle histoire

Lors du [dernier talk](https://youtu.be/yEzKbvbOmTI?si=kYxuvHOrZYWLzhWA){:target="_blank"} de mon ami Thierry Chantier √† Devoxx France 2025, il revient sur l'historique du dialogue homme-machine.
Du pourquoi, en passant au comment, jusqu'√† nous montrer comment le r√©aliser avec une TUI.
Je vous laisse aller le voir pour avoir plus de d√©tails sur cet historique et sur ce qu'est une TUI (Text User Interface) üòâ.
Tout √ßa pour dire que, vous le constaterez, le dialogue homme-machine n'est pas tout jeune, voir il remonte aux ann√©es 60-70 o√π on avait besoin d‚Äôinteragir avec les mainframes (pour les plus jeunes : l'√©quivalent d'une maison remplie de serveurs pour avoir √† peine la puissance de votre t√©l√©phone üòÖ).

Donc oui, arriver √† envoyer nos demandes, nos ordres √† un ordinateur n'est pas une nouveaut√©.
Cela a juste √©volu√© avec le temps, des terminaux passifs √† nos t√©l√©phones, en passant par les sites web et applications sur les ordinateurs.

Quelque soit la technologie et le moment, toutes ces interactions avaient un point commun : passer par une interface con√ßue par une d√©veloppeuse ou un d√©veloppeur avec un chemin d√©terministe duquel on ne pouvait pas d√©vier.

## üí¨ Le langage naturel, la nouvelle fa√ßon d'interagir

Ce qui est tr√®s √©trange au final, c'est que l'on a pass√© toutes ces derni√®res ann√©es √† faire apprendre √† nos utilisatrices et utilisateurs une nouvelle fa√ßon de transcrire leurs besoins √† travers des interfaces graphiques pour revenir √† la base : le langage naturel.

Car oui, qui n'a jamais √©t√© frustr√© de pas pouvoir remplir un champs car trop court, ou avec des valeurs non pr√©vues par les d√©vs ?

> Ca d√©pend, √ßa d√©passe ...

C'est l√†, pour moi, le gros changement avec les LLM (Large Language Models) : nous pouvons simplement demander (avec des fautes en plus üòâ), avec notre langue maternelle, ce dont nous avons besoin et la magie de l'IA fait le reste pour traduire cela et essayer d'en calculer (oui n‚Äôoubliez pas que ce ne sont que des statistiques üòâ) la r√©ponse la plus pertinente.

Si c'est tr√®s impressionnant, avec un chatbot nous n'avons r√©invent√© que la fa√ßon de consommer du contenu web.
Au lieu d'aller sur Wikipedia, des sites webs ou de faire une recherche sur votre moteur de recherche vous demandez et le LLM se charge d'agr√©ger les donn√©es les plus pertinentes pour vous.
Nous sommes donc, selon moi, dans un premier changement dans notre mani√®re de mettre √† disposition du contenu : pas de site web et de formulaires de recherche avec des listes d√©roulantes et autres cases √† cocher.
Ici, on demande simplement dans notre langue pr√©f√©r√©e ce que l'on veut et l'affichage de la r√©ponse (pertinente ou pas) arrive.

Notez ici, que ce mode a quelques inconv√©nients : 
 - üóìÔ∏è la donn√©e utilis√©e est celle du moment de l‚Äôentra√Ænement du mod√®le, cela peut varier de quelques jours √† quelques ann√©es,
 - üßÆ cela reste des statistiques, donc la r√©ponse n'est pas une certitude issue d'un seul contenu, mais certainement une agr√©gation de plusieurs contenus traitant du m√™me sujet,
 - ‚¨á tout ceci est en mode _pull_ : le mod√®le renvoie de la donn√©e en "lecture seule"

Je pr√©f√®re aussi tout de suite aborder un point : je parle de remplacer le front, mais il faut bien afficher la r√©ponse du mod√®le non ?
Oui c'est totalement vrai.
A la diff√©rence que, selon moi, au lieu de d√©velopper plusieurs applications ou sites web je ne d√©veloppe que le chatbot qui va interagir avec le mod√®le.

## ü§Ø Rendre le mod√®le intelligent, du moins actuel

Bon, c'est bien joli mais au final, du pattern CRUD (Create, Read, Update, Delete), ici je vous vends un Read du pauvre avec des donn√©es obsol√®tes et pas forc√©ment valides üòÇ.

Tr√®s vite ce probl√®me, tout le monde l'a constat√©, et derri√®re la hype du moment on s'est rendu compte qu'avoir un super historien du web c'est bien mais que, pour avoir les nouvelles de la veille ou du jour √ßa ne marchait pas tr√®s bien üì∞.

Pour palier √† √ßa, on a vu appara√Ætre deux grands mouvements : le fine tuning et le RAG (Retrieval-Augmented Generation).

Le fine tuning on peut le mettre dans la cat√©gorie je cr√©e mon mod√®le mais en plus simple.
Vous prenez un mod√®le existant et vous l‚Äôentra√Ænez sur des donn√©es qui vous permettent de le sp√©cialiser (par exemple pour les assistants de code avec des donn√©es repr√©sentant uniquement du code).
Cela fonctionne bien pour sp√©cialiser un mod√®le sur des donn√©es qu'il n‚Äôaurait pas vues, mais c'est assez mauvais avec des donn√©es en faible quantit√© qui traiteraient des informations en temps r√©el.

Le RAG est plus prometteur : on s√©lectionne un corpus de donn√©es pertinentes, par exemple la version √©lectronique du journal du jour, et on ajoute ce corpus de donn√©es √† la requ√™te (aussi appel√©e _prompt_) que l'on envoie au mod√®le en lui demandant de les utiliser pour donner sa r√©ponse.
Cela fonctionne plut√¥t bien, mais cela demande d'avoir ces donn√©es dans un format le permettant et oblige certainement un premier filtre pour ne pas avoir √† envoyer trop de donn√©es.
Merci √† la recherche vectorielle qui permet d'avoir une phase de recherche s√©mantique dans les donn√©es √† envoyer √† votre mod√®le.

> üì∫ Je vous conseille l'[excellent talk](https://youtu.be/kceR97PM_3k?si=C4mSewkKDwPaZfQ-){:target="_blank"} de Bertrand Nau qui explique comment fonctionne les bases de donn√©es vectorielles lors du dernier Devoxx France.

On a donc progress√© : 
 - on peut sp√©cialiser un mod√®le pour qu'il soit le plus pertinent possible (mais attention jamais √† 100%)
 - on peut ajouter de la donn√©e pour l'aider √† avoir des donn√©es plus actuelles avant de nous r√©pondre

On a progress√© dans notre refonte de l'IHM, mais on reste √† ne pouvoir faire que de la lecture et il faut avoir pr√©par√© en amont les donn√©es dans le cas du RAG (pour le fine tuning aussi mais de fa√ßon diff√©rente avec des datasets), ce qui peut vite devenir fastidieux et co√ªteux.

> Si vous voulez voir comment impl√©menter du RAG, voici quelques liens d'articles que j'ai √©crits dans le cadre de mon activit√© professionnelle üòâ :
>  - [RAG chatbot using AI Endpoints and LangChain4J](https://blog.ovhcloud.com/rag-chatbot-using-ai-endpoints-and-langchain4j/){:target="_blank"}
>  - [RAG chatbot using AI Endpoints and LangChain](https://blog.ovhcloud.com/rag-chatbot-using-ai-endpoints-and-langchain/){:target="_blank"}

## üß∞ Le function calling, l'IA enfin √† jour et capable d'interagir ?

Le function calling est apparu pour pallier ce probl√®me de : comment faire pour que mon mod√®le soit √† jour avec n'importe quelle donn√©e dont il aurait besoin pour me r√©pondre ?
Et quand je dis n'importe quelle, je ne parle pas que des donn√©es publiques mais aussi celles de votre entreprise ou de vos donn√©es personnelles.
Alors oui, en fait la partie lecture est d√©j√† possible avec le RAG : au lieu de requ√™ter une base de donn√©es vectorielle, rien ne vous emp√™che de faire une requ√™te sur une API pour ajouter cette donn√©e au contexte.

Le function calling va permettre de faire cet ajout de donn√©es mais il va surtout permettre d'appeler n'importe quel outil et de faire n'importe quelle action : Create, Read, Update et Delete.

On y vient √† mon interface Homme - Machine.

> **‚ö†Ô∏è D√®s √† pr√©sent pour la suite de l'article retenez bien une chose : ce n'est JAMAIS et non JAMAIS le mod√®le qui appelle les outils mais bien l'application appelante (celle qui fait la requ√™te au mod√®le) qui le fait. ‚ö†Ô∏è**

Il me semblait utile de faire cette mise au point üòâ.

Le function calling va donc √™tre simple : notre application met √† disposition au mod√®le une liste d'outils que le mod√®le peut conseiller d'appeler avec une liste de param√®tres qui correspond le plus probablement √† ce que souhaite la personne qui utilise le chatbot.
Suite √† cela le mod√®le va ou non indiquer quels outils et param√®tres il faut utiliser, en fonction de la teneur du prompt, √† l'application appelante et c'est elle qui va d√©clencher les appels.

Un petit exemple plus parlant : je souhaite cr√©er un enregistrement `personne` dans ma base de donn√©es (acc√©d√©e par mon application).
Je d√©clare donc une fonction `enregistrerPersonne` qui prend en param√®tre des cha√Ænes de caract√®res repr√©sentant le nom, le pr√©nom et l'email de la personne.
J'indique √† mon mod√®le que cette fonction est disponible, comment elle s'appelle, √† quoi elle sert et quels param√®tres elle utilise.
Suite √† cela je vais pouvoir demander √† mon mod√®le via mon chatbot : `Cr√©er la personne St√©phane Philippart dans la base de donn√©es, son email est foo@bar.com`.

A la suite de cela le mod√®le va pouvoir indiquer en retour √† mon application qu'il faut appeler la fonction `enregistrerPersonne` avec comme param√®tres `"St√©phane", "Philippart", "foo@bar.com"`.
Et l'application va donc se charger de faire l'√©criture en base de donn√©es (et c'est bien l'application et non le mod√®le üòâ).

Bien entendu, on peut imaginer la m√™me chose pour de la mise √† jour ou de la suppression.

A ce stade, on avance dans notre recherche d'avoir une interface utilisateur o√π je suis libre de formuler mes demandes comme je le souhaite sans √™tre dans un carcan d'une interface graphique rigide.
On peut aussi noter d'autres avantages : pas de traductions, pas d'√©volutions quand la base de donn√©es ou les r√®gles m√©tiers changent.

Il reste toujours des probl√®mes, bien s√ªr : 
 - l'appel de la fonction n'est pas d√©terministe : m√™me si vous pouvez jouer sur des param√®tres lors de l'appel √† votre mod√®le (comme la temp√©rature ou le param√®tre top_P par exemple), vous ne serez jamais s√ªr √† 100% qu'il fasse les bons choix ... souvenez-vous des statistiques et tout √ßa üòâ,
 - la s√©curit√© : souhaitez-vous vraiment ne jamais intervenir dans le processus de d√©cisions ? Pour une application de gestion √ßa va encore, pour des d√©cisions plus grave ... üò±. Il existe des m√©canismes de contr√¥les que vous pouvez impl√©menter mais, souvent, ils font appel √† des LLM (m√©canisme de LLM as judge par exemple). A ce stade pour les actions critiques il semble qu'une validation humaine en bout de cha√Æne soit une bonne id√©e üöß.
 - la redondance de code : ma super fonction qui fait une action tr√®s bien, j'ai envie de pouvoir la partager en interne, l√† vous pouvez cr√©er une lib. Mais si vous voulez √©viter de tout d√©porter sur la machine cliente vous allez vite arriver √† la question : mais comment d√©ployer √ßa pour que mon outil soit utilisable de n'importe o√π ü§î ?

> Si vous voulez voir comment impl√©menter du function calling, voici un lien vers un article que j'ai √©crit dans le cadre de mon activit√© professionnelle üòâ : [Using Function Calling with OVHcloud AI Endpoints](https://blog.ovhcloud.com/using-function-calling-with-ovhcloud-ai-endpoints/){:target="_blank"}

## üìö MCP, l'invention des architectures n-tiers par l'IA ?

Le [Model Context Protocol](https://modelcontextprotocol.io/introduction){:target="_blank"} (MCP) est une proposition d'[Anthropic](https://www.anthropic.com/){:target="_blank"} pour palier ce probl√®me de distribution des outils.
Imaginons que je suis un fournisseur connu d'un service permettant de versionner du code et de d√©clarer des issues et des PR üêô ?
Je me rends compte que beaucoup d'entreprises d√©veloppent leurs propres outils pour leurs chatbots leurs permettant de faire les issues, les PR, ...

Qui de mieux que moi pour fournir un tel outil, et le rendre disponible pour mes clients ?

C'est un tr√®s gros r√©sum√© de ce que propose MCP : la cr√©ation d'un protocole vous permettant de r√©cup√©rer, entre autre, la liste des outils d'un ou de plusieurs fournisseurs.
Et ensuite cela fonctionne comme dans le paragraphe pr√©c√©dent pour le function calling, sauf que l'application appelante n'a plus √† coder l'outil c'est le fournisseur via MCP qui, non seulement fournit la liste des outils, mais aussi se charge de les appeler suite √† la r√©ponse du LLM.

Et tout √ßa ... tout le monde s'y engouffre, tout le monde veux exposer son super serveur MCP tout beau !

> üì∫ Vous sentirez une pointe d'ironie dans cette phrase, et c'est vrai üòÜ. Je vous conseille [le talk](https://youtu.be/hICGtUH7K-4?si=i2hoVBkqiI2LrvDk){:target="_blank"} de S√©bastien Blanc lors du dernier Devoxx UK qui d√©mystifie tr√®s bien ce sujet. 

Car oui ... on s'extasie devant une architecture 3 tiers qui a plus de 20 ans ... Interface, logique m√©tier, donn√©es.

Mais attention, loin de moi de dire que ce n'est pas une vrai avanc√©e, le MCP et toutes ses variantes concurrentes apportent, pour moi, la derni√®re pierre pour que l'on repense totalement la fa√ßon dont l'Homme interagit avec la machine.

Bien s√ªr tout n'est pas rose : 
 - il reste des probl√®mes de v√©racit√© (les statistiques encore et encore)
 - la s√©curit√© : √† l'heure actuelle c'est un peu le Far West dans les serveurs MCP, et avant de donner vos identifiants et mots de passes, r√©fl√©chissez √† comment ils sont envoy√©s, stock√©s, utilis√©s, ...

> Si vous voulez voir comment impl√©menter un serveur MCP, voici un lien vers un article que j'ai √©crit dans le cadre de mon activit√© professionnelle üòâ : [Model Context Protocol (MCP) with OVHcloud AI Endpoints](https://blog.ovhcloud.com/model-context-protocol-mcp-with-ovhcloud-ai-endpoints/){:target="_blank"}

## üßê Alors le front est fini, tout comme les d√©vs front ?

Non bien s√ªr, et ce n'est pas le sujet de mon article.
J'esp√®re que vous avez compris ü§ó.

C'est juste, qu'apr√®s ces quelques ann√©es pass√©es dans l'IA et plus particuli√®rement dans la gen AI avec les LLM je me rends compte que l'on se rapproche de plus en plus d'une nouvelle fa√ßon d'√©changer avec nos machines.
Demain vous rajouterez la voix, mais n'est-ce d√©j√† pas le cas avec les assistants personnels ?
La seule diff√©rence est qu'on lui demandera de commander un billet de train plut√¥t que de calculer le temps pour aller √† la gare üòâ.

En dehors des effets d'annonces et de d√©mos faites pour faire briller les yeux, ne vous y trompez pas, il reste des challenges importants √† relever.
La s√©curit√© semble √™tre le plus important.
Mais aussi la consommation induite par une sur-utilisation des gros LLM.

A ce sujet, je pense que le function calling avec ou sans MCP commence √† √™tre une r√©ponse pour diminuer la taille des mod√®les, en les am√©liorant, √† la demande, avec des outils (tient c'est marrant c'est ce qu'est sens√©e faire l'IA avec l'humain non ?).
N'h√©sitez pas non plus √† [aller voir ce que fait](https://k33g.hashnode.dev/){:target="_blank"} Philippe Charri√®re, il donne un nombre impressionnants d'exemples sur le sujet des Small Languages Models (SML) mais aussi sur les diff√©rents sujets abord√©s dans cet article.

Vous l'avez compris, j'esp√®re, le but de cet article n'est pas de vous expliquer dans le d√©tail ce que sont et comment impl√©menter les RAG, MCP et autres fonction calling.
Je vous ai tout de m√™me mis des liens vers d'autres articles que j'ai √©crits qui, j'esp√®re, vous aideront. 

Cet article n'avait pas pour but de r√©volutionner votre fa√ßon de travailler avec l'intelligence artificielle, mais il m'a permis de mettre des mots sur une id√©e qui me trottait en t√™te depuis un moment.

Si il vous a plu, ou m√™me si j'en ai envie, je rajouterai peut √™tre des articles permettant d'illustrer de mani√®re programmatique comment concevoir un chatbot utilisant ces notions.
Vous avez d√©j√† quelques liens pour commencer √† sauter le pas üòâ.

Si vous √™tes arriv√©s jusque l√† merci de m'avoir lu et si il y a des coquilles n'h√©sitez pas √† me faire une [issue ou PR](https://github.com/philippart-s/blog){:target="_blank"} üòä.

