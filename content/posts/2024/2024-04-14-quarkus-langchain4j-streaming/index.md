---
title: "ğŸ¤– Augmente les capacitÃ©s de ton IA avec LangChain4j ğŸ¦œ"
description: "DeuxiÃ¨me partie de la dÃ©couverte de LangChain4j au travers de Quarkus."
link: /2024-04-14-quarkus-langchain4j-streaming
tags: 
  - Quarkus
  - LangChain4J
  - IA
image: robot-ia.jpg
figCaption: "@wildagsx"
author: wilda
---

Je vous propose la suite de l'[article prÃ©cÃ©dent](/2024-04-01-quarkus-langchain4j}) nous ayant permis la dÃ©couverte de [LangChain4j](https://docs.langchain4j.dev/intro/) au travers de [Quarkus](https://quarkus.io/).
> â„¹ï¸ Je vous laisse donc y jeter un oeil pour toute la phase d'installation, de prÃ©requis nÃ©cessaires Ã  la bonne comprÃ©hension de cet article.

Lors de ce premier article, nous avons vu ensemble comment dÃ©velopper notre premier chat bot.
Celui-ci Ã©tait trÃ¨s simple et il fallait attendre que le modÃ¨le distant _fabrique_ l'ensemble de la rÃ©ponse avant de l'avoir en retour.
Pas trÃ¨s pratique et convivial.

Cette fois-ci, je vous propose d'ajouter quelques fonctionnalitÃ©s rendant notre _chat bot_ plus "intelligent".  
On va donc le faire se comporter comme un _chat bot_ normal : streamer sa rÃ©ponse.
On va aussi lui ajouter un peu de contexte afin qu'il connaisse plus de choses essentielles ğŸ˜‰ !


>â„¹ï¸ L'ensemble du code source se trouve dans le repository GitHub [discover-langchain4j](https://github.com/philippart-s/discover-langchain4j)

## ğŸŒŠ Activation du mode streaming

C'est la premiÃ¨re fonctionnalitÃ© que nous allons rajouter : cela permet de rendre le bot plus convivial et de ne pas avoir Ã  attendre sans trop savoir quand il va nous rÃ©pondre ğŸ˜….  
Pour activer cette fonctionnalitÃ©, c'est assez simple : nous allons utiliser [Mutiny](https://quarkus.io/guides/mutiny-primer).
Mais c'est quoi me direz-vous ğŸ¤¨ ?  
En deux mots : cela vous permet d'ajouter une notion d'asynchronisme dans votre dÃ©veloppement et de basculer dans ce que l'on appelle la programmation reactive.  

L'objectif ?
Permettre Ã  notre IA d'envoyer son dÃ©but de rÃ©ponse avant mÃªme d'avoir envoyÃ© l'ensemble de la rÃ©ponse.

### ğŸ”€ Ajout de l'asynchronisme

[Quarkus](https://quarkus.io/) et son extension [quarkus-langchain4j](https://github.com/quarkiverse/quarkus-langchain4j/) vont encore grandement nous aider.

> â„¹ï¸ Nous repartons du code de l'[article prÃ©cÃ©dent](2024-04-01-quarkus-langchain4j}), si vous souhaitez plus de dÃ©tails n'hÃ©sitez pas Ã  vous reporter Ã  l'article ğŸ˜‰.

Nous allons donc modifier notre service _OllamaService.java_ pour qu'il supporte le mode streaming : 

```java
package fr.wilda.quarkus;

import dev.langchain4j.service.SystemMessage;
import dev.langchain4j.service.UserMessage;
import io.quarkiverse.langchain4j.RegisterAiService;
import io.smallrye.mutiny.Multi;

// AI service bean registration
@RegisterAiService
public interface OllamaAIService {
  
  // Context message
  @SystemMessage("You are an AI assistant.")  
  // Prompt customisation
  @UserMessage("Answer as best possible to the following question: { question}. The answer must be in a style of a virtual assistant and use emoji.")
  String askAQuestion(String question);

  // Context message
  @SystemMessage("You are an AI assistant.")  
  // Prompt customisation
  @UserMessage("Answer as best possible to the following question: { question}. The answer must be in a style of a virtual assistant and use emoji.")
  // Multi use is enough to activate streaming mode
  Multi<String> askAQuestionStreamingMode(String question);

}
```

âš ï¸ Notez-bien ici l'utilisation de l'interface `io.smallrye.mutiny.Multi` qui permet "d'activer" le mode streaming.
L'extension se charge de l'activer lors de ses requÃªtes au modÃ¨le ğŸ˜‰.  
A noter que seul le type `String` est, pour l'instant, supportÃ© pour le mode streaming mais des Ã©tudes d'Ã©volutions sont en cours âš ï¸

Maintenant, nous allons faire Ã©voluer notre partie API pour qu'elle puisse profiter de cette arrivÃ©e d'informations au fil de l'eau.

### ğŸ§© Modification de l'API Rest

L'idÃ©e est de proposer une ressource qui va Ãªtre _streamÃ©e_ au fur et Ã  mesure.
On met donc Ã  jour la  classe `AIAssistant` qui expose le endpoint `hal9000`.

```java
package fr.wilda.quarkus;

import org.jboss.resteasy.reactive.RestQuery;
import io.smallrye.mutiny.Multi;
import jakarta.inject.Inject;
import jakarta.ws.rs.GET;
import jakarta.ws.rs.POST;
import jakarta.ws.rs.Path;

// Endpoint root path
@Path("hal9000")
public class AIAssistant {

  // AI Service injection to use it later
  @Inject
  OllamaAIService ollamaAIService;

  // ask resource exposition with POST method
  @Path("ask")
  @POST
  public String ask(String question) {
    // Call the Mistral AI chat model
    return ollamaAIService.askAQuestion(question);
  }

  // Stream response
  @Path("streaming")
  @GET
  public Multi<String> streaming(@RestQuery("question") String question) {
    // Call the Mistral AI chat model
    return ollamaAIService.askAQuestionStreamingMode(question);
  }
}
```

On a rajoutÃ© une ressource `streaming` qui retourne `Multi<String>`, Quarkus fera le reste.  
Ensuite il suffit d'aller sur l'URL `http://localhost:8080/hal9000/streaming?question="What is the answer to the Ultimate Question of Life,the Universe, and Everything?"`

{% include video id="T0LbsThvaRY" provider="youtube"}

## ğŸ‘Œ Ajoutons un peu de contexte

L'intelligence des diffÃ©rents modÃ¨les que l'on utilise dÃ©pend grandement des donnÃ©es utilisÃ©es mais aussi de quand date la derniÃ¨re _indexation_ de ces donnÃ©es (oui je sais que ce n'est pas le terme standard mais Ã§a permet de se comprendre ğŸ˜‰).

Essayons ce prompt, voulez-vous : _Can you tell me more about StÃ©phane Philippart?_  

VoilÃ  la rÃ©ponse donnÃ©e:  
```
ğŸ‘‹ Hello! StÃ©phane Philippart is a renowned figure in the tech industry, particularly in Belgium.
ğŸ‡§ğŸ‡ªğŸ’» He's known for his expertise in Information Technology (IT) and Digital Transformation. StÃ©phane has spent over two decades in the tech sector, holding various key positions in leading companies.
ğŸ•’âœ¨ His achievements include co-founding several successful startups and contributing significantly to their growth. 
He's also a sought-after speaker at industry events, sharing his insights on digital transformation trends._
```

Flatteur mais trÃ¨s loin de la rÃ©alitÃ© non ğŸ¤¨ ?

### ğŸ—ƒï¸ Le RAG Ã  la rescousse

Rien d'Ã©tonnant dans cette rÃ©ponse :
 - je ne suis pas trÃ¨s connu et donc pas Ã©tonnant de pas avoir beaucoup de donnÃ©es sur moi
 - mon prÃ©nom, assez commun, fait que l'hallucination avec d'autres _StÃ©phane_ n'est pas Ã©tonnante
 - la plupart des Ã©lÃ©ments publiques datent de 2-3 ans et souvent les modÃ¨les ont Ã©tÃ© entraÃ®nÃ©s sur des donnÃ©es plus anciennes

En quoi le RAG va nous aider ?  
Le RAG ou encore _Retrieval Augmented Generation_ va vous permettre d'ajouter des donnÃ©es non connues de votre modÃ¨le pour lui donner un contexte qui correspond Ã  votre domaine fonctionnel.
Pour ajouter ce contexte on va prendre une source de donnÃ©es, par exemple des fichiers, puis les transformer dans un format permettant une recherche par similitudes, dans ce cas une base de donnÃ©es vectorielle.  

Une fois ce contenu ajoutÃ©, lors de requÃªtes envoyÃ©es au modÃ¨le, celui-ci va pouvoir se baser sur ces donnÃ©es supplÃ©mentaires pour contextualiser sa rÃ©ponse.

> Le RAG se diffÃ©rencie d'une autre technique, le transfert learning, par le fait que l'on utilise le modÃ¨le tel quel en lui ajoutant des donnÃ©es / du contexte.  
> Le transfert learning va consister Ã  rÃ©-entrainer un modÃ¨le pour un use case diffÃ©rent de celui dâ€™origine, cela demande donc plus de calculs et une phase dâ€™entraÃ®nement lÃ  oÃ¹ le RAG se fait au runtime.

### ğŸ“ƒ Les donnÃ©es Ã  rajouter

Je vous l'ai dit, on peut choisir plusieurs types de donnÃ©es.
Dans mon cas, je vais choisir un fichier texte.

```
StÃ©phane Philippart is a world-renowned developer advocate in the field of cloud computing. He is also the CTO of Tours' biggest meetup.
Tours is well known for its famous rillettes.
```

### ğŸ§© Activation du RAG

Pour utiliser le RAG il va falloir deux choses : 
 - un moyen de transformer nos donnÃ©es en vecteurs
 - un moyen d'ajouter ces donnÃ©es dans la chaÃ®ne d'interrogation de notre modÃ¨le

LÃ  encore, vous vous en doutez, Quarkus et LangChain4j vont nous Ãªtre d'un grand secours !

```xml
<!-- To add RAG capabilities -->
<dependency>
    <groupId>io.quarkiverse.langchain4j</groupId>
    <artifactId>quarkus-langchain4j-easy-rag</artifactId>
    <version>0.13.0</version>
</dependency>  

<!-- Inner process embedding model -->
<dependency>
    <groupId>dev.langchain4j</groupId>
    <artifactId>langchain4j-embeddings-all-minilm-l6-v2-q</artifactId>
    <version>0.30.0</version>
</dependency>
```
Merci Ã  l'extension Quarkus qui, par le simple ajout de la dÃ©pendance `quarkus-langchain4j-easy-rag`, active le mode [Easy RAG](https://docs.quarkiverse.io/quarkus-langchain4j/dev/easy-rag.html).

Notez l'ajout de la dÃ©pendance `dev.langchain4j:langchain4j-embeddings-all-minilm-l6-v2-q` qui me permet d'avoir un [_embedding model_ in process](https://docs.quarkiverse.io/quarkus-langchain4j/dev/in-process-embedding.html).
J'ai choisi cela plutÃ´t que d'utiliser le endpoint d'embedding de Mistral car en mode Ollama il n'est pas accessible avec LangChain4j.

Au chargement de l'application on voit que le mode RAG est activÃ© avec les bonnes donnÃ©es : 
```bash
2024-05-05 20:12:23,987 INFO  [io.qua.lan.eas.run.EasyRagRecorder] (Quarkus Main Thread) Ingesting documents from path: ./src/main/resources/rag/, path matcher = glob:**, recursive = true
```

RÃ©essayons de demander au modÃ¨le s'il connaÃ®t des choses sur _StÃ©phane Philippart_ !  
Lâ€™appel Ã  l'URL `http://localhost:8080/hal9000/streaming?question="Can you tell me more about StÃ©phane Philippart?"` donne cette fois : 
```
ğŸŒ Hey there! StÃ©phane Philippart,ğŸ‘¨â€ğŸ’» is a globally recognized developer advocate in the cloud computing domain! *claps* 
His expertise is highly sought after, making him a key figure in this innovative field.ğŸ’¡Moreover, StÃ©phane holds an impressive title as the CTO of Tours' biggest meetup!
ğŸ¢ This city in France, famously known for its scrumptious rillettes,ğŸ¥“ğŸ˜‹ is where he makes a significant impact`
```

Pas forcÃ©ment plus vrai mais mieux quand mÃªme non ? ğŸ˜‰

**âš ï¸ Ce petit exemple doit vous faire allumer quelques alertes dans votre cerveau : on ne peut dÃ©finitivement pas faire confiance aux rÃ©sultats d'une IA que l'on peut si facilement biaiser en quelques lignes de codes âš ï¸**

# En conclusion

J'espÃ¨re que vous avez pu, simplement, vous rendre compte comme il est assez facile avec Quarkus et LangChain4j d'ajouter des capacitÃ©s Ã  notre application de _chat bot_.
Je vais continuer Ã  ajouter quelques autres choses plus ou moins utiles dans les articles suivants ğŸ˜‰.

Si vous Ãªtes arrivÃ©s jusque lÃ  merci de m'avoir lu et si il y a des coquilles n'hÃ©sitez pas Ã  me faire une [issue ou PR](https://github.com/philippart-s/blog) ğŸ˜Š.

Merci Ã  ma relectrice, Fanny, qui vous permet de lire cet article sans avoir trop les yeux qui saignent ğŸ˜˜.

L'ensemble des sources des exemples est disponible dans le repository GitHub [langchain4j-discovery](https://github.com/philippart-s/discover-langchain4j).