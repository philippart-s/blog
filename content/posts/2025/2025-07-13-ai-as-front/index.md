---
title: "ğŸ¤– Et si les LLM Ã©taient le nouveau front ? ğŸ¨"
description: "On nous promet la fin des dÃ©veloppeuses et dÃ©veloppeurs, mais si au final l'IA n'Ã©tait qu'une nouvelle faÃ§on de faire du front ?"
link: /2025-07-13-ai-as-front
tags: 
  - IA
  - Front
  - LLM
image: cover.jpg
figCaption: "@wildagsx"
author: wilda
---

L'intelligence artificielle va-t-elle remplacer toute une partie de mÃ©tiers que l'on connaÃ®t actuellement ?
C'est une des thÃ©ories que l'on entend le plus souvent ces derniers temps.
Et plus particuliÃ¨rement dans notre microcosme, celui du dÃ©veloppement, n'est-on pas en train de scier la branche sur laquelle on est assis ?

En toute honnÃªtetÃ© je n'en sais rien ğŸ˜… ... tout Ã§a pour Ã§a me direz-vous !

Ce qui a l'air plutÃ´t sÃ»r c'est que cela risque de changer la faÃ§on dont nous exerÃ§ons notre mÃ©tier mais aussi la maniÃ¨re que les utilisatrices et utilisateurs interagissent avec les applications que nous crÃ©ons.
Et c'est lÃ  que je veux en venir : le front va-t-il disparaÃ®tre ?

DerriÃ¨re cette question un brin racoleuse, on pourrait la nuancer : l'interface Homme - Machine va-t-elle se transformer avec l'arrivÃ©e massive des chatbots boostÃ©s Ã  l'IA ?

## ğŸ–¥ï¸ Le dialogue Homme - Machine, une vielle histoire

Lors du [dernier talk](https://youtu.be/yEzKbvbOmTI?si=kYxuvHOrZYWLzhWA) de mon ami Thierry Chantier Ã  Devoxx France 2025, il revient sur l'historique du dialogue homme-machine.
Du pourquoi, en passant au comment, jusqu'Ã  nous montrer comment le rÃ©aliser avec une TUI.
Je vous laisse aller le voir pour avoir plus de dÃ©tails sur cet historique et sur ce qu'est une TUI (Text User Interface) ğŸ˜‰.
Tout Ã§a pour dire que, vous le constaterez, le dialogue homme-machine n'est pas tout jeune, voir il remonte aux annÃ©es 60-70 oÃ¹ on avait besoin dâ€™interagir avec les mainframes (pour les plus jeunes : l'Ã©quivalent d'une maison remplie de serveurs pour avoir Ã  peine la puissance de votre tÃ©lÃ©phone ğŸ˜…).

Donc oui, arriver Ã  envoyer nos demandes, nos ordres Ã  un ordinateur n'est pas une nouveautÃ©.
Cela a juste Ã©voluÃ© avec le temps, des terminaux passifs Ã  nos tÃ©lÃ©phones, en passant par les sites web et applications sur les ordinateurs.

Quelque soit la technologie et le moment, toutes ces interactions avaient un point commun : passer par une interface conÃ§ue par une dÃ©veloppeuse ou un dÃ©veloppeur avec un chemin dÃ©terministe duquel on ne pouvait pas dÃ©vier.

## ğŸ’¬ Le langage naturel, la nouvelle faÃ§on d'interagir

Ce qui est trÃ¨s Ã©trange au final, c'est que l'on a passÃ© toutes ces derniÃ¨res annÃ©es Ã  faire apprendre Ã  nos utilisatrices et utilisateurs une nouvelle faÃ§on de transcrire leurs besoins Ã  travers des interfaces graphiques pour revenir Ã  la base : le langage naturel.

Car oui, qui n'a jamais Ã©tÃ© frustrÃ© de pas pouvoir remplir un champs car trop court, ou avec des valeurs non prÃ©vues par les dÃ©vs ?

> Ca dÃ©pend, Ã§a dÃ©passe ...

C'est lÃ , pour moi, le gros changement avec les LLM (Large Language Models) : nous pouvons simplement demander (avec des fautes en plus ğŸ˜‰), avec notre langue maternelle, ce dont nous avons besoin et la magie de l'IA fait le reste pour traduire cela et essayer d'en calculer (oui nâ€™oubliez pas que ce ne sont que des statistiques ğŸ˜‰) la rÃ©ponse la plus pertinente.

Si c'est trÃ¨s impressionnant, avec un chatbot nous n'avons rÃ©inventÃ© que la faÃ§on de consommer du contenu web.
Au lieu d'aller sur Wikipedia, des sites webs ou de faire une recherche sur votre moteur de recherche vous demandez et le LLM se charge d'agrÃ©ger les donnÃ©es les plus pertinentes pour vous.
Nous sommes donc, selon moi, dans un premier changement dans notre maniÃ¨re de mettre Ã  disposition du contenu : pas de site web et de formulaires de recherche avec des listes dÃ©roulantes et autres cases Ã  cocher.
Ici, on demande simplement dans notre langue prÃ©fÃ©rÃ©e ce que l'on veut et l'affichage de la rÃ©ponse (pertinente ou pas) arrive.

Notez ici, que ce mode a quelques inconvÃ©nients : 
 - ğŸ—“ï¸ la donnÃ©e utilisÃ©e est celle du moment de lâ€™entraÃ®nement du modÃ¨le, cela peut varier de quelques jours Ã  quelques annÃ©es,
 - ğŸ§® cela reste des statistiques, donc la rÃ©ponse n'est pas une certitude issue d'un seul contenu, mais certainement une agrÃ©gation de plusieurs contenus traitant du mÃªme sujet,
 - â¬‡ tout ceci est en mode _pull_ : le modÃ¨le renvoie de la donnÃ©e en "lecture seule"

Je prÃ©fÃ¨re aussi tout de suite aborder un point : je parle de remplacer le front, mais il faut bien afficher la rÃ©ponse du modÃ¨le non ?
Oui c'est totalement vrai.
A la diffÃ©rence que, selon moi, au lieu de dÃ©velopper plusieurs applications ou sites web je ne dÃ©veloppe que le chatbot qui va interagir avec le modÃ¨le.

## ğŸ¤¯ Rendre le modÃ¨le intelligent, du moins actuel

Bon, c'est bien joli mais au final, du pattern CRUD (Create, Read, Update, Delete), ici je vous vends un Read du pauvre avec des donnÃ©es obsolÃ¨tes et pas forcÃ©ment valides ğŸ˜‚.

TrÃ¨s vite ce problÃ¨me, tout le monde l'a constatÃ©, et derriÃ¨re la hype du moment on s'est rendu compte qu'avoir un super historien du web c'est bien mais que, pour avoir les nouvelles de la veille ou du jour Ã§a ne marchait pas trÃ¨s bien ğŸ“°.

Pour palier Ã  Ã§a, on a vu apparaÃ®tre deux grands mouvements : le fine tuning et le RAG (Retrieval-Augmented Generation).

Le fine tuning on peut le mettre dans la catÃ©gorie je crÃ©e mon modÃ¨le mais en plus simple.
Vous prenez un modÃ¨le existant et vous lâ€™entraÃ®nez sur des donnÃ©es qui vous permettent de le spÃ©cialiser (par exemple pour les assistants de code avec des donnÃ©es reprÃ©sentant uniquement du code).
Cela fonctionne bien pour spÃ©cialiser un modÃ¨le sur des donnÃ©es qu'il nâ€™aurait pas vues, mais c'est assez mauvais avec des donnÃ©es en faible quantitÃ© qui traiteraient des informations en temps rÃ©el.

Le RAG est plus prometteur : on sÃ©lectionne un corpus de donnÃ©es pertinentes, par exemple la version Ã©lectronique du journal du jour, et on ajoute ce corpus de donnÃ©es Ã  la requÃªte (aussi appelÃ©e _prompt_) que l'on envoie au modÃ¨le en lui demandant de les utiliser pour donner sa rÃ©ponse.
Cela fonctionne plutÃ´t bien, mais cela demande d'avoir ces donnÃ©es dans un format le permettant et oblige certainement un premier filtre pour ne pas avoir Ã  envoyer trop de donnÃ©es.
Merci Ã  la recherche vectorielle qui permet d'avoir une phase de recherche sÃ©mantique dans les donnÃ©es Ã  envoyer Ã  votre modÃ¨le.

> ğŸ“º Je vous conseille l'[excellent talk](https://youtu.be/kceR97PM_3k?si=C4mSewkKDwPaZfQ-) de Bertrand Nau qui explique comment fonctionne les bases de donnÃ©es vectorielles lors du dernier Devoxx France.

On a donc progressÃ© : 
 - on peut spÃ©cialiser un modÃ¨le pour qu'il soit le plus pertinent possible (mais attention jamais Ã  100%)
 - on peut ajouter de la donnÃ©e pour l'aider Ã  avoir des donnÃ©es plus actuelles avant de nous rÃ©pondre

On a progressÃ© dans notre refonte de l'IHM, mais on reste Ã  ne pouvoir faire que de la lecture et il faut avoir prÃ©parÃ© en amont les donnÃ©es dans le cas du RAG (pour le fine tuning aussi mais de faÃ§on diffÃ©rente avec des datasets), ce qui peut vite devenir fastidieux et coÃ»teux.

> Si vous voulez voir comment implÃ©menter du RAG, voici quelques liens d'articles que j'ai Ã©crits dans le cadre de mon activitÃ© professionnelle ğŸ˜‰ :
>  - [RAG chatbot using AI Endpoints and LangChain4J](https://blog.ovhcloud.com/rag-chatbot-using-ai-endpoints-and-langchain4j/)
>  - [RAG chatbot using AI Endpoints and LangChain](https://blog.ovhcloud.com/rag-chatbot-using-ai-endpoints-and-langchain/)

## ğŸ§° Le function calling, l'IA enfin Ã  jour et capable d'interagir ?

Le function calling est apparu pour pallier ce problÃ¨me de : comment faire pour que mon modÃ¨le soit Ã  jour avec n'importe quelle donnÃ©e dont il aurait besoin pour me rÃ©pondre ?
Et quand je dis n'importe quelle, je ne parle pas que des donnÃ©es publiques mais aussi celles de votre entreprise ou de vos donnÃ©es personnelles.
Alors oui, en fait la partie lecture est dÃ©jÃ  possible avec le RAG : au lieu de requÃªter une base de donnÃ©es vectorielle, rien ne vous empÃªche de faire une requÃªte sur une API pour ajouter cette donnÃ©e au contexte.

Le function calling va permettre de faire cet ajout de donnÃ©es mais il va surtout permettre d'appeler n'importe quel outil et de faire n'importe quelle action : Create, Read, Update et Delete.

On y vient Ã  mon interface Homme - Machine.

> **âš ï¸ DÃ¨s Ã  prÃ©sent pour la suite de l'article retenez bien une chose : ce n'est JAMAIS et non JAMAIS le modÃ¨le qui appelle les outils mais bien l'application appelante (celle qui fait la requÃªte au modÃ¨le) qui le fait. âš ï¸**

Il me semblait utile de faire cette mise au point ğŸ˜‰.

Le function calling va donc Ãªtre simple : notre application met Ã  disposition au modÃ¨le une liste d'outils que le modÃ¨le peut conseiller d'appeler avec une liste de paramÃ¨tres qui correspond le plus probablement Ã  ce que souhaite la personne qui utilise le chatbot.
Suite Ã  cela le modÃ¨le va ou non indiquer quels outils et paramÃ¨tres il faut utiliser, en fonction de la teneur du prompt, Ã  l'application appelante et c'est elle qui va dÃ©clencher les appels.

Un petit exemple plus parlant : je souhaite crÃ©er un enregistrement `personne` dans ma base de donnÃ©es (accÃ©dÃ©e par mon application).
Je dÃ©clare donc une fonction `enregistrerPersonne` qui prend en paramÃ¨tre des chaÃ®nes de caractÃ¨res reprÃ©sentant le nom, le prÃ©nom et l'email de la personne.
J'indique Ã  mon modÃ¨le que cette fonction est disponible, comment elle s'appelle, Ã  quoi elle sert et quels paramÃ¨tres elle utilise.
Suite Ã  cela je vais pouvoir demander Ã  mon modÃ¨le via mon chatbot : `CrÃ©er la personne StÃ©phane Philippart dans la base de donnÃ©es, son email est foo@bar.com`.

A la suite de cela le modÃ¨le va pouvoir indiquer en retour Ã  mon application qu'il faut appeler la fonction `enregistrerPersonne` avec comme paramÃ¨tres `"StÃ©phane", "Philippart", "foo@bar.com"`.
Et l'application va donc se charger de faire l'Ã©criture en base de donnÃ©es (et c'est bien l'application et non le modÃ¨le ğŸ˜‰).

Bien entendu, on peut imaginer la mÃªme chose pour de la mise Ã  jour ou de la suppression.

A ce stade, on avance dans notre recherche d'avoir une interface utilisateur oÃ¹ je suis libre de formuler mes demandes comme je le souhaite sans Ãªtre dans un carcan d'une interface graphique rigide.
On peut aussi noter d'autres avantages : pas de traductions, pas d'Ã©volutions quand la base de donnÃ©es ou les rÃ¨gles mÃ©tiers changent.

Il reste toujours des problÃ¨mes, bien sÃ»r : 
 - l'appel de la fonction n'est pas dÃ©terministe : mÃªme si vous pouvez jouer sur des paramÃ¨tres lors de l'appel Ã  votre modÃ¨le (comme la tempÃ©rature ou le paramÃ¨tre top_P par exemple), vous ne serez jamais sÃ»r Ã  100% qu'il fasse les bons choix ... souvenez-vous des statistiques et tout Ã§a ğŸ˜‰,
 - la sÃ©curitÃ© : souhaitez-vous vraiment ne jamais intervenir dans le processus de dÃ©cisions ? Pour une application de gestion Ã§a va encore, pour des dÃ©cisions plus grave ... ğŸ˜±. Il existe des mÃ©canismes de contrÃ´les que vous pouvez implÃ©menter mais, souvent, ils font appel Ã  des LLM (mÃ©canisme de LLM as judge par exemple). A ce stade pour les actions critiques il semble qu'une validation humaine en bout de chaÃ®ne soit une bonne idÃ©e ğŸš§.
 - la redondance de code : ma super fonction qui fait une action trÃ¨s bien, j'ai envie de pouvoir la partager en interne, lÃ  vous pouvez crÃ©er une lib. Mais si vous voulez Ã©viter de tout dÃ©porter sur la machine cliente vous allez vite arriver Ã  la question : mais comment dÃ©ployer Ã§a pour que mon outil soit utilisable de n'importe oÃ¹ ğŸ¤” ?

> Si vous voulez voir comment implÃ©menter du function calling, voici un lien vers un article que j'ai Ã©crit dans le cadre de mon activitÃ© professionnelle ğŸ˜‰ : [Using Function Calling with OVHcloud AI Endpoints](https://blog.ovhcloud.com/using-function-calling-with-ovhcloud-ai-endpoints/)

## ğŸ“š MCP, l'invention des architectures n-tiers par l'IA ?

Le [Model Context Protocol](https://modelcontextprotocol.io/introduction) (MCP) est une proposition d'[Anthropic](https://www.anthropic.com/) pour palier ce problÃ¨me de distribution des outils.
Imaginons que je suis un fournisseur connu d'un service permettant de versionner du code et de dÃ©clarer des issues et des PR ğŸ™ ?
Je me rends compte que beaucoup d'entreprises dÃ©veloppent leurs propres outils pour leurs chatbots leurs permettant de faire les issues, les PR, ...

Qui de mieux que moi pour fournir un tel outil, et le rendre disponible pour mes clients ?

C'est un trÃ¨s gros rÃ©sumÃ© de ce que propose MCP : la crÃ©ation d'un protocole vous permettant de rÃ©cupÃ©rer, entre autre, la liste des outils d'un ou de plusieurs fournisseurs.
Et ensuite cela fonctionne comme dans le paragraphe prÃ©cÃ©dent pour le function calling, sauf que l'application appelante n'a plus Ã  coder l'outil c'est le fournisseur via MCP qui, non seulement fournit la liste des outils, mais aussi se charge de les appeler suite Ã  la rÃ©ponse du LLM.

Et tout Ã§a ... tout le monde s'y engouffre, tout le monde veux exposer son super serveur MCP tout beau !

> ğŸ“º Vous sentirez une pointe d'ironie dans cette phrase, et c'est vrai ğŸ˜†. Je vous conseille [le talk](https://youtu.be/hICGtUH7K-4?si=i2hoVBkqiI2LrvDk) de SÃ©bastien Blanc lors du dernier Devoxx UK qui dÃ©mystifie trÃ¨s bien ce sujet. 

Car oui ... on s'extasie devant une architecture 3 tiers qui a plus de 20 ans ... Interface, logique mÃ©tier, donnÃ©es.

Mais attention, loin de moi de dire que ce n'est pas une vrai avancÃ©e, le MCP et toutes ses variantes concurrentes apportent, pour moi, la derniÃ¨re pierre pour que l'on repense totalement la faÃ§on dont l'Homme interagit avec la machine.

Bien sÃ»r tout n'est pas rose : 
 - il reste des problÃ¨mes de vÃ©racitÃ© (les statistiques encore et encore)
 - la sÃ©curitÃ© : Ã  l'heure actuelle c'est un peu le Far West dans les serveurs MCP, et avant de donner vos identifiants et mots de passes, rÃ©flÃ©chissez Ã  comment ils sont envoyÃ©s, stockÃ©s, utilisÃ©s, ...

> Si vous voulez voir comment implÃ©menter un serveur MCP, voici un lien vers un article que j'ai Ã©crit dans le cadre de mon activitÃ© professionnelle ğŸ˜‰ : [Model Context Protocol (MCP) with OVHcloud AI Endpoints](https://blog.ovhcloud.com/model-context-protocol-mcp-with-ovhcloud-ai-endpoints/)

## ğŸ§ Alors le front est fini, tout comme les dÃ©vs front ?

Non bien sÃ»r, et ce n'est pas le sujet de mon article.
J'espÃ¨re que vous avez compris ğŸ¤—.

C'est juste, qu'aprÃ¨s ces quelques annÃ©es passÃ©es dans l'IA et plus particuliÃ¨rement dans la gen AI avec les LLM je me rends compte que l'on se rapproche de plus en plus d'une nouvelle faÃ§on d'Ã©changer avec nos machines.
Demain vous rajouterez la voix, mais n'est-ce dÃ©jÃ  pas le cas avec les assistants personnels ?
La seule diffÃ©rence est qu'on lui demandera de commander un billet de train plutÃ´t que de calculer le temps pour aller Ã  la gare ğŸ˜‰.

En dehors des effets d'annonces et de dÃ©mos faites pour faire briller les yeux, ne vous y trompez pas, il reste des challenges importants Ã  relever.
La sÃ©curitÃ© semble Ãªtre le plus important.
Mais aussi la consommation induite par une sur-utilisation des gros LLM.

A ce sujet, je pense que le function calling avec ou sans MCP commence Ã  Ãªtre une rÃ©ponse pour diminuer la taille des modÃ¨les, en les amÃ©liorant, Ã  la demande, avec des outils (tient c'est marrant c'est ce qu'est sensÃ©e faire l'IA avec l'humain non ?).
N'hÃ©sitez pas non plus Ã  [aller voir ce que fait](https://k33g.hashnode.dev/) Philippe CharriÃ¨re, il donne un nombre impressionnants d'exemples sur le sujet des Small Languages Models (SML) mais aussi sur les diffÃ©rents sujets abordÃ©s dans cet article.

Vous l'avez compris, j'espÃ¨re, le but de cet article n'est pas de vous expliquer dans le dÃ©tail ce que sont et comment implÃ©menter les RAG, MCP et autres fonction calling.
Je vous ai tout de mÃªme mis des liens vers d'autres articles que j'ai Ã©crits qui, j'espÃ¨re, vous aideront. 

Cet article n'avait pas pour but de rÃ©volutionner votre faÃ§on de travailler avec l'intelligence artificielle, mais il m'a permis de mettre des mots sur une idÃ©e qui me trottait en tÃªte depuis un moment.

Si il vous a plu, ou mÃªme si j'en ai envie, je rajouterai peut Ãªtre des articles permettant d'illustrer de maniÃ¨re programmatique comment concevoir un chatbot utilisant ces notions.
Vous avez dÃ©jÃ  quelques liens pour commencer Ã  sauter le pas ğŸ˜‰.

Si vous Ãªtes arrivÃ©s jusque lÃ  merci de m'avoir lu et si il y a des coquilles n'hÃ©sitez pas Ã  me faire une [issue ou PR](https://github.com/philippart-s/blog) ğŸ˜Š.
